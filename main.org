#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t broken-links:nil c:nil creator:nil
#+options: d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t num:nil p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:nil toc:nil todo:t |:t
# #+TITLE: Combining High-Throughput Computations, Surrogate Models, and
# #+TITLE: Genetic Algorithms for Discovering Novel Halide Perovskites
# #+AUTHOR: Panayotis Manganaris
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 29.0.50 (Org mode 9.5.2)
#+cite_export: natbib
#+latex_class: revtex
#+latex_class_options: [aip, jmp, amsmath, amssymb]
#+latex_header:
#+latex_header_extra:
#+description:
#+keywords:
#+subtitle:
#+latex_compiler: pdflatex
#+date: \today
#+SETUPFILE: ~/org/revtex_header.org
#+INCLUDE: ./frontmatter.org
#+PROPERTY: header-args:jupyter-python :session mrg :kernel mrg :pandoc org :async yes
#+PROPERTY: header-args :results scalar drawer :eval never-export :exports results
#+begin_abstract
We report on the details of creating models of halide perovskite properties based on composition and derived
descriptors. The primary objective of these models is to eventually recommend perovskite alloy compositions
corresponding to targeted properties. Here targets are chosen to yield high photovoltaic (PV) performance. So, we focus
on models of the electronic band gap. We leverage the Purdue University nanoHUB, an NSF-funded, Purdue-hosted
computational repository, to host literate reproducible notebooks documenting our model development workflow
[[cite:&manganaris-2022-mrs-comput]]. We thus enable the scientific community to utilize our approach for modeling
performance targets for a wider range of promising compounds.

We explore a variety of machine learning (ML) models for the prediction of Perovskite bandgap. A rigorously optimized
Random Forest Regressor (RFR), a Gaussian Process (GP) Regressor, and a Sure Independent Screening and Sparsifying
Operator[[cite:&ouyang-2018-sisso]] (SISSO) regressor.

Approximately 1500 physical and synthetic records spanning various experimental fidelities and alloy schemes are used in
model development. All experiments are conducted for one of ~500 perovskite compositions. ~1400 experiments are
performed computationally using Density Functional Theory (DFT), ~100 are physical measurements obtained from published
literature[[cite:&almora-2020-devic-perfor;&jiang-2006-predic-lattic;&briones-2021-accel-lattic]].

1. 500 PBE relaxations -> PBE Density of States (DoS) calculation
2. 300 HSE06 relaxations -> HSE06 DoS
3. 300 HSE06 relaxations -> HSE06 + Spin-Orbit Coupling (SOC) DoS
4. 300 PBE relaxations -> HSE06 + SOC DoS
5. 100 experimental band gap measurements

Our models are based primarily on composition information. We implement generic feature extraction by parsing a string
encoding the ABX_3 perovskite formula corresponding to each record. The resulting 14-dimensional composition vector is
easily obtained for experimental and synthetic data alike. This is a sufficient predictor variable, nonetheless we
continue. Secondarily, we also examine 36 additional predictor variables computed as linear combinations of these
compositions and certain elemental properties obtained from the trusted Mendeleev databases [[cite:&mentel-2014]]. Finally,
additional fidelity features are one-hot-encoded with the aim of improving model accuracy. In future work, we anticipate
adding descriptors based on phase and structural information.

We finally compare the band gap models based on this basic 55 dimensional descriptor and models based on an engineered
domain we produced to improve model efficiency, performance, and interpretability.
#+end_abstract
* COMMENT dependencies
#+INCLUDE: /home/panos/Documents/manuscripts/DFT+ML+feature_engineering/dev_dependencies.org
#+begin_src jupyter-python
  import re
  from plotly.subplots import make_subplots

  import matplotlib.pyplot as plt
  plt.rc("figure", facecolor='w')
  from spyglass import parityplot, biplot
  import plotly.express as px
  import plotly.io as pio
  import plotly.graph_objects as go

  pub_template = go.layout.Template()
  pub_template.data = pio.templates['simple_white'].data
  pub_template.layout = pio.templates['simple_white'].layout
  pub_template.layout.font = dict(family='arial narrow', size=15)

  pio.templates.default = pub_template

#+end_src

#+RESULTS:
:results:
:end:

* COMMENT load data
#+INCLUDE: /home/panos/Documents/manuscripts/DFT+ML+feature_engineering/load_full_domain.org
** for illustrating the domain
#+begin_src jupyter-python
  target=['bg_eV']
  Y = my.dropna(subset=target)
  # use mm, or mc/mp + categorical features
  X = mm.reindex(index=Y.index)
  # X = mm[[mc.columns.to_list + ["LoT"]]]
#+end_src

#+RESULTS:
:results:
:end:

** create preprocessing pipeline
#+begin_src jupyter-python
  # composition vectors for preprocessing
  # comp_features = X.select_dtypes(np.number).columns[
  #     X.select_dtypes(np.number).columns.str.match(r"\('[ABX]', '[^E]\w{,2}'\)")
  # ]
  # composition vectors for preprocessing
  comp_features = mc.columns

  # site-avg properties for preprocessing
  prop_features = mp.columns

  # categorical properties for preprocessing
  cat_features = mm.select_dtypes('object').columns

  # define preprocessing
  fillna = SimpleImputer(strategy="constant", fill_value=0.0)
  mkratio = Normalizer(norm='l1')
  mknormal = StandardScaler()
  mkbound = MinMaxScaler(feature_range=(0,1), clip=False) #not statistical, should work for anything

  comp_transformer = mkpipe(fillna, mkratio, mkbound)
  prop_transformer = mkpipe(fillna, mknormal, mkbound)
  cat_transformer = ohe(handle_unknown="ignore")

  preprocessor = colt(
      transformers=[
          ("comp", comp_transformer, comp_features),
          ("prop", prop_transformer, prop_features),
          ("cat", cat_transformer, cat_features),
      ]
  )
#+end_src

#+RESULTS:
:results:
:end:

** fit data and create visualization df
#+begin_src jupyter-python
  preprocessor.fit(X)
#+end_src

#+RESULTS:
:results:
#+begin_example
  ColumnTransformer(transformers=[('comp',
                                   Pipeline(steps=[('simpleimputer',
                                                    SimpleImputer(fill_value=0.0,
                                                                  strategy='constant')),
                                                   ('normalizer',
                                                    Normalizer(norm='l1')),
                                                   ('minmaxscaler',
                                                    MinMaxScaler())]),
                                   Index(['('A', 'Cs')', '('A', 'FA')', '('A', 'K')', '('A', 'MA')',
         '('A', 'Rb')', '('B', 'Ba')', '('B', 'Ca')', '('B', 'Ge')',
         '('B', 'Pb')', '('B', 'Sn')', '('B', 'Sr')', '('X',...
         '('X', 'ion_rad_â„«')', '('X', 'Boiling_Point_K')',
         '('X', 'Melting_Point_K')', '('X', 'dens_g/cc')', '('X', 'at_wt_u')',
         '('X', 'El_aff_kJ/mol')', '('X', 'IonE_kJ/mol')',
         '('X', 'Heat_of_fusion_kJ/mol')', '('X', 'Heat_of_vap_kJ/mol')',
         '('X', 'En')', '('X', 'at_num')', '('X', 'period')'],
        dtype='object')),
                                  ('cat', OneHotEncoder(handle_unknown='ignore'),
                                   Index(['LoT'], dtype='object'))])
#+end_example
:end:

#+begin_src jupyter-python
  XX = pd.DataFrame(preprocessor.transform(X),
                   columns=preprocessor.get_feature_names_out(),
                   index=X.index)
#+end_src

#+RESULTS:
:results:
:end:

** targets
no target transformations yet
#+begin_src jupyter-python
  YY = Y
#+end_src

#+RESULTS:
:results:
:end:

** subset as necessary
#+begin_src jupyter-python
  YY = YY#[Y.LoT=='PBE']
  # use mm, or mc/mp + categorical features
  XX = XX#[y.LoT=='PBE']
  # X = mm[[mc.columns.to_list + ["LoT"]]]
#+end_src

#+RESULTS:
:results:
:end:

* DONE Introduction
:PROPERTIES:
:CLASS: unnumbered
:END:
:STATUSLOG:
- State "DONE"       from "TODO"       [2022-12-21 Wed 16:56]
:END:
** DONE Multi-Fidelity Learning 
:STATUSLOG:
- State "DONE"       from "TODO"       [2022-12-21 Wed 11:26]
- State "DONE"       from "TODO"       [2022-09-13 Tue 12:51]
:END:
The state of the art in materials modeling favors [[Acrfull:gnn][Graph Neural Network
(GNN)]][[cite:&chen-2019-graph-networ;&choudhary-2021-atomis-line;&xie-2018-cryst-graph]]
architectures. These [[gls:dl][deep learning]] models have sufficient flexibility
to capture the continuous variability in relative positions of
crystals and molecules. They are extremely effective models, but they
are difficult to use with physical materials. Accurately
characterizing structures at a level of atomic granularity cannot be
achieved even with state of the art 3D Electron Tomography
techniques[[cite:&ercius-2015-elect-tomog]]. Yet, characterization of
chemical composition is a well established practice i.e using X-ray
spectroscopy.

Graph convolutional neural networks can power more accurate
structure-target predictions at multiple
fidelities[[cite:&chen-2020-multi-fidel]] by performing [[Gls:mtl][Multi-Task
Learning]] (MTL). For instance, this multiple-fidelity machine learning
technique can infer the relationships between more plentiful [[ACRshort:pbe][PBE]] [[ACRshort:gga][GGA]]
data and rarer but more accurate [[ACRshort:hse][HSE06]] data based on a shared set of
predicting [[gls:ft][features]]. This relationship, if sufficiently general, can
be used to reliably extrapolate from known points on the [[ACRshort:pbe][PBE]] co-domain
to the unknown [[ACRshort:hse][HSE]] co-domain. Of course, while this is implemented
successfully in neural networks, the concept holds for any model
architecture that can simultaneously regress multidimensional targets
which do not need to constitute one rectangular data structure.

Additionally, there are alternative [[gls:mtl][multi-task learning]] approaches
that can be implemented on the domain side. This circumvents the
requirement for flexibility in encoding the co-domain, making it
possible to use a single target regression methods to learn rules for
multiple outcomes that vary depending on a categorical variable
representing the fidelity. In general, the problem of accurately
modeling low availability, high fidelity targets is approached using
MTL to learn regressions on datasets compiled either along X or Y from
measurements taken at multiple [[gls:lot][levels of theory]].

Semi-supervised
learning[[cite:&chapelle-2006-semi-super-learn;&lee-2013-pseud-label]] is
a competing set of methods achieving similar outcomes.

We will employ the domain-side approach where the largest, lowest
fidelity component of our dataset consists of [[acrfull:dft][density functional
theory (DFT)]] band gap predictions made at the [[acrfull:gga][generalized gradient
approximation (GGA)]] [[acrfull:pbe][Perdew-Burke-Ernzerhof (PBE)]] [[gls:lot][level of theory]]. On
the other end, the smallest and highest fidelity subset of the sample
consists of experimental measurements of physical devices collected
from the literature.

** DONE Dataset Overview
:STATUSLOG:
- State "DONE"       from "TODO"       [2022-12-21 Wed 12:16]
:END:
*** DONE Perovskite Band Gaps
:STATUSLOG:
- State "DONE"       from "TODO"       [2022-12-21 Wed 12:16]
:END:
# need to introduce the significance of predicting band gaps more
# effectively
we aim to accurately predict performance-relevant [[Acrfull:hap][Halide perovskite
(HaP)]] band gaps which are strongly predictive of photovoltaic
performance[[cite:&mannodi-kanakkithodi-2019-compr-comput]].

Furthermore, using MTL modeling we aim to predict the experimentally
measured band gaps of compounds that have only been simulated to
date. Our fidelity hierarchy climbs from [[ACRshort:dft][DFT]] simulations performed
using the basic [[acrshort:pbe][PBE]] [[acrshort:gga][GGA]] functional, to results obtained from physical
experiments aggregated in
literature[[cite:&almora-2020-devic-perfor;&kim-2014-cdses-nanow;&swanson-2017-co-sublim]]
see table ref:tbl:LoTs.

While we acknowledge the advantages of [[acrshort:gnn][GNNs]], we aim to express band
gap as functions primarily of the perovskite composition. It is known
this effort is suboptimal especially as the octahedral arrangement of
perovskites is most relevant to their electronic structure,
Nevertheless, a strong understanding of the influence of chemical
composition on performance will continue to be a priority as it is
expected to aid in [[Future Work][inverse design]] and in [[Feature Engineering][Feature Engineering]].

#+begin_src jupyter-python :post wraptbl(*this*, c="label:tbl:LoTs Density Functionals vs Sample Counts")
  my.LoT.value_counts().to_frame()
#+end_src

#+RESULTS:
:results:
 
#+CAPTION: label:tbl:LoTs Density Functionals vs Sample Counts 
|              |  LoT |
|--------------+------|
| PBErel       |  517 |
| HSErel(SOC)  |  299 |
| HSErel       |  297 |
| HSE-PBE(SOC) |  244 |
| EXP          |   44 |
|--------------+------|
| Total        | 1401 |
#+TBLFM: $2=vsum(@I..@II)
:end:

A detailed analysis of this combined hybrid organic-inorganic and
purely inorganic [[acrshort:hap][HaP]] [[ACRshort:dft][DFT]] dataset is covered in a prior article by
[[citet:&yang-2022-high-throug]] and in section [[DFT Details]].

Naturally, the statistics obtained from each fidelity vary (fig
ref:fig:bg_dist). This is the primary challenge we will address in
section [[Methods]].

#+begin_src jupyter-python :post wrap(*this*, c="label:fig:bg_dist Variability in band gap per level of theory")
  p = px.histogram(Y, x="bg_eV", color="LoT", barmode="overlay")
  p.update_xaxes(title="Band Gap [eV]")
  p.show('svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 300pt
#+CAPTION: label:fig:bg_dist Variability in band gap per level of theory
#+attr_org: :width 700
[[file:./.ob-jupyter/2ff00cfe78bc5c9b2f1f7e489f0bb2c6a0643052.svg]]
:end:

*** DONE Sampling
:STATUSLOG:
- State "DONE"       from              [2022-09-13 Tue 12:51]
:END:
The simulations for each [[gls:lot][level of theory]] are performed on some number
of members to a fixed subset of the total 37785 compositions that can
be combinatorially generated in a 2x2x2 perovskite supercell when
allowing /at most/ single-site alloying with our 14 constituent
candidates for 3 sites (table ref:tbl:site_tbl).

#+caption: label:tbl:site_tbl ABX_3 Chemical Domain
| /      | <  |    |    |    |    |    |
| A-site | [[ACRshort:ma][MA]] | [[ACRshort:fa][FA]] | Cs | Rb | K  |    |
| B-site | Pb | Sn | Ge | Ba | Sr | Ca |
| X-site | I  | Br | Cl |    |    |    |

Within this sample space, we try to maintain a balance in the share of
samples that represent each one of the "[[gls:cmix][cardinal mixing]]"
categories. Additionally, within each mix we try to maintain a
reasonable balance of purely inorganic samples versus hybrid
organic-inorganic samples. See figure ref:fig:lot_mix_org. See section
[[Methods]] for details on how these categories were utilized in model
development.

The design of this dataset provides an opportunity to assess the
ability of our models to extrapolate with respect to alloying scheme
as well as [[gls:lot][level of theory]]. It also provides an opportunity to
investigate the statistical impact of constituent compounds on
perovskite property prediction. See section [[Results and Discussion]].

#+begin_src jupyter-python :post wrap(*this*, c="\label{fig:lot_mix_org}Share by count of total data apportioned from each experimental subcategory")
  p = px.sunburst(my, path=['LoT','mix','org'])
  p.update_layout(
      margin=dict(l=0, r=0, t=0, b=0),
  )
  llist = p.data[0].labels
  vlist = [v if not re.search(r'[0-9]', l) else "" for l,v in zip(p.data[0].labels, p.data[0].values)]
  p.data[0].labels=np.array(list(zip(llist,vlist)))

  p.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 300pt
#+CAPTION: label{fig:lot_mix_org}Share by count of total data apportioned from each experimental subcategory
#+attr_org: :width 700
[[file:./.ob-jupyter/714175a3d13e3c523962f378572a20cc21500c4d.svg]]
:end:

** DONE Model Optimization
:STATUSLOG:
- State "DONE"       from "TODO"       [2022-12-21 Wed 16:56]
:END:
# not every scrap of data collected in the experiments was included in the sample
# standard cleaning and deduping operations need only be mentioned

The rigorous [[gls:hp][hyper-parameter]] Optimization (HPO) of any feature
engineering and modeling pipeline is a problem discussed extensively
in the literature. HPO approaches can be broadly separated into
exhaustive and efficient optimization strategies
[[cite:&yang-2020-hyper-optim]]. We use a two-stage procedure for
selecting the best model parameters.

The first stage is an exhaustive grid-search over diversely sampled
parameter space. Each combination of parameters instantiates a model
which is then fit to each of a set of stratified training subsets
generated by a K=3 [[gls:kfs][K-fold split]] [[gls:cv][cross-validation]] strategy. Every
fitted model is subsequently tested against the [[gls:cv][cross-validation]] test
sets and a suite of regression scoring metrics are applied to each
member category simultaneously using a custom scikit-learn score
adapter.

The scoring metrics we choose vary by model architecture. See the [[HPO
Summary Tables]].

The grid search is then narrowed to a high performance quadrant of the
search space by the model evaluator based on recommendations made by a
simple entropy minimization algorithm.

# implemented in the "yogi"
# supplementary package under the yogi.model_selection.butler module --
# see documentation for the various grid-narrowing strategies available.

In general, the recommended grid quickly eliminates under-performing
settings based on the sample probability of a setting appearing in a
set of finalists according to the scoring rankings. The selection
score is additionally influenced by a weighted sum of the scoring
ranks allowing for considerably tuning the selection criterion.  For
best results, a few different grid spaces should be explored to
corroborate eliminations.

After the recommendation is made, the granularity of the grid is
increased in the remaining ambiguous parameters and the process is
repeated. In general, no more than 2 or 3 exhaustive searches are
needed over a given set of grids.

Past this point, continuously variable hyper parameters can be
individually optimized using validation curves.

* RESULTS AND DISCUSSION
:PROPERTIES:
:CUSTOM_ID: results-and-discussion
:CLASS: unnumbered
:END:
** Domain analysis
*** most explanatory basic features
*** engineered features
naturally, by utilizing GPR in conjunction with these new feature
combinations, we can create models with the efficiency of SISSO while
obtaining uncertainty estimates.
** Best Models
- band gap parity plots
- summary scores
  - best scores
  - best extrapolative ability

** model introspection
*** linear models
To illustrate, the coefficients that define the linear combination do
not find much use in X-site elements. however, the B site elements
contribute much more to the band gap on average, which is consistent
with our physical understanding.

Curiously, A site elements also prove to be relevant.

Likewise, with respect to the generality measure conducted earlier, it
seems the presence of individual elements is far more predictive of
the total band gap than mix status. This would explain why X-site and
A-site alloys are sufficient to predict the band gaps of B-site alloys,
despite those groups containing no B-site alloys themselves, they do
contain a representative sample of B-site elements.
*** 
** Screening
:PROPERTIES:
:CUSTOM_ID: screeding
:CLASS: unnumbered
:END:
A set of high throughput screening criteria has been previously
developed[[cite:mannodi-kanakkithodi-2022-data-driven]]. We apply the
band gap criterion to the predictions made the by these enhanced models.

*** TODO Compare
screening results to the previous batch produced by older models.

** Summary
:PROPERTIES:
:CUSTOM_ID: summary
:CLASS: unnumbered
:END:

GPR performs best and

** CANCELED COMMENT Genetic algorithm results
:STATUSLOG:
- State "CANCELED"   from              [2022-09-11 Sun 19:28] \\
  GA will have a dedicated paper
:END:
- Fitness score vs generation, stability only.

- Fitness score vs generation, stability + band gap.

- Fitness score vs generation, stability + band gap + SLME.

- Performance of GA with NN/GPR/RFR and with comp/elem/comp+elem.

Validation using new DFT calculations on 10 compositions in 4x4x4
supercell. DFT vs ML properties + band structure + absorption spectra.

* METHODS
:PROPERTIES:
:CUSTOM_ID: methods
:CLASS: unnumbered
:END:
Several machine learning architectures are rigorously optimized with
regard to both generality over the domains of Perovskite compositions
and site-averaged atomic properties and generality over the domain of
alloy classifications.

In order to control for the classification biases potentially acting
on the parameter space of regression models, nine metrics are used to
evaluate the performance of each model over all alloy types at every
stage of the hyper-parameter optimization simultaneously. Only models
that perform uniformly well on all alloy classes are selected.

Validation curves are computed for hyper-parameters to which a given
model is particularly sensitive.

** TODO Objectives
Our objectives are two fold. First, we aim to accurately predict
performance-relevant Perovskite band gaps.

We will follow a multi-fidelity approach, where the bulk of affordable
low level-of-theory data will inform and improve the extrapolative
ability of models trained on higher fidelity measurements.

Our fidelity hierarchy climbs from simulations performed using the
basic PBE functional, to results obtained from physical experiments
aggregated in literature[[cite:&almora-2020-devic-perfor]].

# a lot of that was redundant

We aim to express these variables as functions of the perovskite
composition. Schemes for incorporating structural information will be
developed in future work. Nevertheless, a strong understanding of the
influence of chemical composition on performance will continue to be a
priority as it is expected to aid both in [[Feature Engineering]] and in
screening the combinatorial chemical space for viable high-entropy
compounds.

Second, we hope to better understand the average physical impacts
of 1) site-specific alloying and 2) using organic molecules in the
Perovskite superstructure.

These goals are not entirely separate from the first goal of
expressing various properties as functions of composition, but they
can be more simply approached as problems of addressing dependencies
in the data statistics. Our model development will test the hypothesis
that formula that fall within one of these classifications will share
some distributed qualities with others that fit their classification.

** TODO Considerations
We expect perovskites of a given alloy class and of a given
hybrid-organic/inorganic status will perform significantly differently
with respect to a particular application compared to perovskites of a
another class or status. We attempt to make models that reasonably
explain this high entropy diversity by utilizing the low entropy
mixing represented in our sample.

We do this by training each model using two test/train splits. First,
the optimal model parameters are chosen for their performance under a
random split. A minimum of 3-fold cross-validation is performed for
every set of model parameters that is considered. Finally, the
optimized model's ability to extrapolate is tested by training/testing
on splits determined with a [[gls:gkf][groupwise K-fold]] splitting strategy.

Two separate cross validation schemes are employed at each stage of
the design process.

First, the sample set is shuffled once and split to mitigate the
models tendency to fit on sample order, then, stratified K-folds are
generated in manner consistent with the classification of each
sample. However, this fold is not used in a classification problem,
the regressor is trained on the subsets of each class, and it's
ability to extrapolate is independently metered on each validation
fold consisting of members of the other classes.

Second, the ability for a model trained on samples belonging to one
class/status to extrapolate to samples of another class/status is
tested as well. The samples again are shuffled and split. then the
training set is separated using a grouping K-fold split strategy.

Per architecture, a model is instantiated using the -- in aggregate --
best performing parameters. These models are finally validated against
the test sets originally split off from the sample in *both their
extrapolative ability and consistency across groups.*
** TODO DFT Details
:PROPERTIES:
:CUSTOM_ID: dft-details
:CLASS: unnumbered
:END:
The current work is focused on cubic phase compounds only.
*** TODO VASP

*** TODO Levels of Theory
The largest table of 490 records contains computed electronic
properties. Each is obtained for a unique composition using static
Density Functional Theory (DFT) simulations (relaxation, electronic)
conducted at a PBE level of theory. Additionally, 299 of the
compositions tested in the first 490 records are fully simulated at an
HSE level of theory. Another *SUBSET* of these are also examined,
following PBE relaxation, using HSE with Spin Orbit Coupling (SOC).

Each simulated structure is made in two ways. Once with the GGA-PBE
functional and once with the HSE06 functional. For structure, band
gaps are obtained using a static band structure calculation performed
at the same and at higher levels-of-theory. Specifically, ~300 of the
same compounds underwent HSE06 bandstructure computations, both with
and without spin-orbit coupling (SOC), for a total of approximately
900 experiments with enhanced accuracy.

SOC is performed for better better electronic properties.

*** TODO Featurization of Chemistries
The largest subdivision of ~1400 compounds correspond to a series of
optoelectronic properties simulations performed using density
functional theory (DFT). The simulated experiments are performed on
~500 pseudo-cubic ABX_3 supercells obtained by geometry
optimization. Each cell demonstrates mixed compositions at none or one
of each of the A, B, or X sites. See Figure.

For \alpha total A-site constituents represented in the whole
database, \beta total B-site constituents, and \gamma total X-site
constituents, we provide a python tool which robustly coverts the
composition string of each data point into a \(\alpha + \beta +
\gamma\) dimensional composition vector. In the case of our dataset
description [[cite:&yang-2022-high-throug]] \(\alpha + \beta + \gamma =
14\).

it is easy to make composition based multi-fidelity
predictors. Involving structure is a challenge for experimentally
collected data because experimental measurements have to be accurately
matched to structure graphs, which generally must be either
exhaustively validated or generated using specialized equipment.

** TODO Feature Engineering
*** The Basic Feature Space
- 14 dimensional composition vectors extracted from chemical formula
- 36 dimensional site-averaged property space computed from composition space
- categorical dimension one-hot-encoding level of theory
  - PBE on PBE
  - HSE on HSE
  - HSE+SOC on HSE
  - HSE+soc on PBE
- models of band gap trained on union of features
  - Linear
  - RFR
  - GPR

*** The Engineered Feature Space
- SISSO
*** the tools
- converting formula strings to vectors
- converting vectors to structures

** TODO Model Training Procedure
*** TODO partitioning
A stratified shuffle split is used to make partitions. This split
preserves the proportion of each [[gls:lot][level of theory]] in the test and train
partitions, which helps with the final model evaluation.

- all decisions about model optimization will be made using only the
  dedicated training partition
- The test partition will be reserved until a final model pipeline is
  parametrized and fit
- the predictions made on the test partition will either confirm or
  deny the model's ability to work outside of the training domain

*** TODO cross validation strategy
**** using Learning Curves
Cross-validation within the training set is the only way of checking
the generality of models during the grid search. Identifying the
validation split size is necessary to obtain an understanding of how
much data is needed to train a model that can generalize.

More data offers better chances. However, the smaller the split, the
longer and more expensive the loop training becomes, e.g. 10-fold
splits makes for 10 sample scores at each partition size. Meaning, 90%
of the training set is used for actual training and the remaining 10%
is used for validation and this is repeated 10 times.

Shuffling is performed prior to generating each fold. The shuffle is
seeded with a deterministic random state to ensure scores are
comparable across partition size

**** RFR results
**** GPR results
*** TODO Hyper-Parameter Optimization
**** performance with diverse samples
**** performance in extrapolation
**** grid search with diverse samples
**** validation curves
* FUTURE WORK
:PROPERTIES:
:CUSTOM_ID: future
:CLASS: unnumbered
:END:
- broaden model domain to include alternative phases

- improve accuracy of surrogate model -- RFR/GPR/NN?
  - incorporate much more experimental data
    [[cite:&jacobsson-2021-open-acces;&briones-2021-accel-lattic]]
  - implement delta learning strategy to utilize more indicators
    besides SLME/PCE

Results suggest that either alternative semi-supervised learning
strategies or much more experimental data is needed to improve the
quality of experimental-fidelity predictions.

Our first objective is to utilize much more experimental data in the
construction of regressions.

An ongoing goal will be growing a database of experimentally examined
perovskite photovoltaic prototypes will well defined structures.

- utilize an active learning approach leveraging GPR models to
  extensively grow the cubic perovskites dataset

The GA fitness function should account for uncertainty in the
surrogate model. GA recommendations can be tuned to explore
uncertainty. Testing these recommendations with DFT forms a loop that
improves the ML models incrementally with exposure to new data,
thereby efficiently discovering the best possible cubic perovskite
compounds.

** identify novel compounds
already, screening has been used to successfully identify some curious options

what more can I add to this?

** utilize convolutional graph neural networks
enabling structure->target models
- MEGnet scalable graph networks for polymorph-sensitive property
  prediction[[cite:&chen-2019-graph-networ]]
- ALIGNN[[cite:&choudhary-2021-atomis-line]]
- CGCNN

Extending models to perform effectively on non-cubic Perovskite phases
is best enabled by the sophisticated graph neural networks being
developed for exactly this purpose.

Already, multi-fidelity "MFGnet" models are being developed under the graph
learning paradigm[[cite:&chen-2020-multi-fidel]].

** Software Tools
A couple of libraries are in development for easing the aggregation,
accessing, sharing, and analysis of this data. The current database is
packaged in "cmcl" at [[http://github.com/PanayotisManganaris/cmcl]] under
the tag v0.1.5. In this early stage of development, cmcl strives to
provide an "inquisitive" interface to perovskite composition feature
computers in the style of the pandas API. At its current stage, it has
been useful for extracting composition vectors from the formula
strings identifying each compound.

Model development and feature extraction is performed using python and
SciKit Learn v1.2. A library of model evaluation tools is being
maintained in the "yogi" repository at
[[http://github.com/PanayotisManganaris/yogi]]

* ACKNOWLEDGMENTS
:PROPERTIES:
:CUSTOM_ID: acknowledgments
:CLASS: unnumbered
:END:
We acknowledge funding from the US Department of Energy SunShot program
under contract #DOE DEEE005956. Use of the Center for Nanoscale
Materials, an Office of Science user facility, was supported by the U.S.
Department of Energy, Office of Science, Office of Basic Energy
Sciences, under Contract No. DE-AC02-06CH11357. We gratefully
acknowledge the computing resources provided on Bebop, a
high-performance computing cluster operated by the Laboratory Computing
Resource Center at Argonne National Laboratory. This research used
resources of the National Energy Research Scientific Computing Center, a
DOE Office of Science User Facility supported by the Office of Science
of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231.
MYT would like to acknowledge support from the U.S. Department of
Energy, Office of Science, Office of Workforce Development for Teachers
and Scientists (WDTS) under the Science Undergraduate Laboratory
Internship (SULI) program. MJD was was supported by the U. S. Department
of Energy , Office of Basic Energy Sciences, Division of Chemical
Sciences, Geosciences, and Biosciences, under Contract No.
DE-AC02-06CH11357.

** Author Contributions
:PROPERTIES:
:CUSTOM_ID: author-contributions
:CLASS: unnumbered
:END:
A.M.K. conceived the idea. A.M.K. and J.Y. performed the DFT
computations. P.T.M. Trained ML models. All authors contributed to the
discussion and writing of the manuscript.

** Data Availability
:PROPERTIES:
:CUSTOM_ID: data-availability
:CLASS: unnumbered
:END:
DFT data and ML models are available from the corresponding author
upon reasonable request. All documents are tracked in the
https://github.com/PanayotisManganaris/manusciprt--multifidelity-dft-ml.git
online repository

** Additional Information
:PROPERTIES:
:CUSTOM_ID: additional-information
:CLASS: unnumbered
:END:
The authors declare no competing financial or non-financial interests.

Correspondence and requests for materials should be addressed to A.M.K.
(email:amannodi@purdue.edu).

* 
:PROPERTIES:
:CUSTOM_ID: references
:CLASS: unnumbered
:END:
bibliographystyle:aipnum4-2
bibliography:~/org/bibliotex/bibliotex.bib

* APPENDIX
:PROPERTIES:
:CUSTOM_ID: APPENDIX
:CLASS: unnumbered
:END:
** Learning Curves
band gap scores vs training set size

Notice that the error metrics are negated so that, consistently with
the R^2 and ev scores, the greater the number, the better the model
performs.
** Generality Tests
** HPO Summary Tables
*** RFR
*** GPR


#+name: glossary
| label | term             | definition                                                                                                               |
|-------+------------------+--------------------------------------------------------------------------------------------------------------------------|
| cmix  | cardinal mixing  | Describes perovskite alloys where no more than one of the A, B, or X sites is occupied by multiple possible constituents |
| prtn  | partition        | Portion of sample data reserved for a purpose in model development                                                       |
| cv    | cross-validation | Method for gathering statistics on the abilities of a model to fit to the parent partition                               |
| kfs   | K-fold split     | Data partition divided into K arbitrary groups for use in cross-validation schemes                                       |
| gkf   | groupwise K-fold | Data partition divided into K-folds where each fold corresponds to a category label                                      |
| lot   | level of theory  | Refers to the rank of a [[ACRshort:dft][DFT]] functional in the hierarchy of phenomenological comprehensiveness. A proxy for accuracy.     |

#+name: acronyms
| label | abbreviation | full form                                     |
|-------+--------------+-----------------------------------------------|
| vasp  | VASP         | Vienna Ab initio Simulation Package           |
| qmml  | QM/ML        | quantum mechanics machine learning            |
| dft   | DFT          | density functional theory                     |
| gga   | GGA          | generalized gradient approximation            |
| pbe   | PBE          | Perdew-Burke-Ernzerhof                        |
| hse   | HSE          | Heyd-Scuseria-Ernzerhof                       |
| ma    | MA           | Methylammonium                                |
| fa    | FA           | Formamidinium                                 |
| hap   | HaP          | halide perovskite                             |
| pca   | PCA          | principal component analysis                  |
| tsne  | t-SNE        | t-distributed stochastic neighbor embedding   |
| umap  | UMAP         | uniform manifold approximation and projection |
