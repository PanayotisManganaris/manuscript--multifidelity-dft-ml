#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t broken-links:nil c:nil creator:nil
#+options: d:(not "STATUSLOG") date:t e:t email:nil f:t inline:t num:nil p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:("TODO" "DONE" "NEXT") tex:t timestamp:t title:nil toc:nil todo:nil |:t
# #+TITLE: Combining High-Throughput Computations, Surrogate Models, and
# #+TITLE: Genetic Algorithms for Discovering Novel Halide Perovskites
# #+AUTHOR: Panayotis Manganaris
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 29.0.50 (Org mode 9.5.2)
#+cite_export:
#+latex_class: revtex
#+latex_class_options: [aip, jmp, amsmath, amssymb, nofootinbib]
#+latex_header:\usepackage[version=4]{mhchem}
#+latex_header:\usepackage{listings}
#+latex_header:\usepackage{glossaries}
#+latex_header:\usepackage[automake]{glossaries-extra}
#+latex_header_extra:
#+description:
#+keywords:
#+subtitle:
#+latex_compiler: pdflatex
#+date: \today
#+SETUPFILE: ~/org/revtex_header.org
#+INCLUDE: ./frontmatter.org
#+PROPERTY: header-args:jupyter-python :session mrg :kernel mrg :pandoc org :async yes
#+PROPERTY: header-args :results scalar drawer :eval never-export :exports results
#+begin_abstract
We report on the details of creating models of halide perovskite properties based on composition and derived
descriptors. The primary objective of these models is to eventually recommend perovskite alloy compositions
corresponding to targeted properties. Here targets are chosen to yield high photovoltaic (PV) performance. So, we focus
on models of the electronic band gap. We leverage the Purdue University nanoHUB, an NSF-funded, Purdue-hosted
computational repository, to host literate reproducible notebooks documenting our model development workflow
[[cite:&manganaris-2022-mrs-comput]]. We thus enable the scientific community to utilize our approach for modeling
performance targets for a wider range of promising compounds.

We explore a variety of machine learning (ML) models for the prediction of Perovskite bandgap. A rigorously optimized
Random Forest Regressor (RFR), a Gaussian Process (GP) Regressor, and a Sure Independent Screening and Sparsifying
Operator[[cite:&ouyang-2018-sisso]] (SISSO) regressor.

Approximately 1500 physical and synthetic records spanning various experimental fidelities and alloy schemes are used in
model development. All experiments are conducted for one of ~500 perovskite compositions. ~1400 experiments are
performed computationally using Density Functional Theory (DFT), ~100 are physical measurements obtained from published
literature[[cite:&almora-2020-devic-perfor;&jiang-2006-predic-lattic;&briones-2021-accel-lattic]].

1. 500 PBE relaxations -> PBE Density of States (DoS) calculation
2. 300 HSE06 relaxations -> HSE06 DoS
3. 300 HSE06 relaxations -> HSE06 + Spin-Orbit Coupling (SOC) DoS
4. 300 PBE relaxations -> HSE06 + SOC DoS
5. 100 experimental band gap measurements

Our models are based primarily on composition information. We implement generic feature extraction by parsing a string
encoding the ABX_3 perovskite formula corresponding to each record. The resulting 14-dimensional composition vector is
easily obtained for experimental and synthetic data alike. This is a sufficient predictor variable, nonetheless we
continue. Secondarily, we also examine 36 additional predictor variables computed as linear combinations of these
compositions and certain elemental properties obtained from the trusted Mendeleev databases [[cite:&mentel-2014]]. Finally,
additional fidelity features are one-hot-encoded with the aim of improving model accuracy. In future work, we anticipate
adding descriptors based on phase and structural information.

We finally compare the band gap models based on this basic 55 dimensional descriptor and models based on an engineered
domain we produced to improve model efficiency, performance, and interpretability.
#+end_abstract
* COMMENT dependencies
#+INCLUDE: /home/panos/Documents/manuscripts/DFT+ML+feature_engineering/dev_dependencies.org
#+begin_src jupyter-python
  import re
  from plotly.subplots import make_subplots

  import matplotlib.pyplot as plt
  plt.rc("figure", facecolor='w')
  from spyglass import parityplot, biplot
  import plotly.express as px
  import plotly.io as pio
  import plotly.graph_objects as go

  pub_template = go.layout.Template()
  pub_template.data = pio.templates['simple_white'].data
  pub_template.layout = pio.templates['simple_white'].layout
  pub_template.layout.font = dict(family='arial narrow', size=15)

  pio.templates.default = pub_template

#+end_src

#+RESULTS:
:results:
:end:

* COMMENT load data
#+INCLUDE: /home/panos/Documents/manuscripts/DFT+ML+feature_engineering/load_full_domain.org
** for illustrating the domain
#+begin_src jupyter-python
  target=['bg_eV']
  Y = my.dropna(subset=target)
  # use mm, or mc/mp + categorical features
  X = mm.reindex(index=Y.index)
  # X = mm[[mc.columns.to_list + ["LoT"]]]
#+end_src

#+RESULTS:
:results:
:end:

** create preprocessing pipeline
#+begin_src jupyter-python
  # composition vectors for preprocessing
  # comp_features = X.select_dtypes(np.number).columns[
  #     X.select_dtypes(np.number).columns.str.match(r"\('[ABX]', '[^E]\w{,2}'\)")
  # ]
  # composition vectors for preprocessing
  comp_features = mc.columns

  # site-avg properties for preprocessing
  prop_features = mp.columns

  # categorical properties for preprocessing
  cat_features = mm.select_dtypes('object').columns

  # define preprocessing
  fillna = SimpleImputer(strategy="constant", fill_value=0.0)
  mkratio = Normalizer(norm='l1')
  mknormal = StandardScaler()
  mkbound = MinMaxScaler(feature_range=(0,1), clip=False) #not statistical, should work for anything

  comp_transformer = mkpipe(fillna, mkratio, mkbound)
  prop_transformer = mkpipe(fillna, mknormal, mkbound)
  cat_transformer = ohe(handle_unknown="ignore")

  preprocessor = colt(
      transformers=[
          ("comp", comp_transformer, comp_features),
          ("prop", prop_transformer, prop_features),
          ("cat", cat_transformer, cat_features),
      ]
  )
#+end_src

#+RESULTS:
:results:
:end:

** fit data and create visualization df
#+begin_src jupyter-python
  preprocessor.fit(X)
#+end_src

#+RESULTS:
:results:
#+begin_example
  ColumnTransformer(transformers=[('comp',
                                   Pipeline(steps=[('simpleimputer',
                                                    SimpleImputer(fill_value=0.0,
                                                                  strategy='constant')),
                                                   ('normalizer',
                                                    Normalizer(norm='l1')),
                                                   ('minmaxscaler',
                                                    MinMaxScaler())]),
                                   Index(['('A', 'Cs')', '('A', 'FA')', '('A', 'K')', '('A', 'MA')',
         '('A', 'Rb')', '('B', 'Ba')', '('B', 'Ca')', '('B', 'Ge')',
         '('B', 'Pb')', '('B', 'Sn')', '('B', 'Sr')', '('X',...
         '('X', 'ion_rad_â„«')', '('X', 'Boiling_Point_K')',
         '('X', 'Melting_Point_K')', '('X', 'dens_g/cc')', '('X', 'at_wt_u')',
         '('X', 'El_aff_kJ/mol')', '('X', 'IonE_kJ/mol')',
         '('X', 'Heat_of_fusion_kJ/mol')', '('X', 'Heat_of_vap_kJ/mol')',
         '('X', 'En')', '('X', 'at_num')', '('X', 'period')'],
        dtype='object')),
                                  ('cat', OneHotEncoder(handle_unknown='ignore'),
                                   Index(['LoT'], dtype='object'))])
#+end_example
:end:

#+begin_src jupyter-python
  XX = pd.DataFrame(preprocessor.transform(X),
                   columns=preprocessor.get_feature_names_out(),
                   index=X.index)
#+end_src

#+RESULTS:
:results:
:end:

** targets
no target transformations yet
#+begin_src jupyter-python
  YY = Y
#+end_src

#+RESULTS:
:results:
:end:

** subset as necessary
#+begin_src jupyter-python
  YY = YY#[Y.LoT=='PBE']
  # use mm, or mc/mp + categorical features
  XX = XX#[y.LoT=='PBE']
  # X = mm[[mc.columns.to_list + ["LoT"]]]
#+end_src

#+RESULTS:
:results:
:end:

* DONE Introduction
:PROPERTIES:
:CLASS: unnumbered
:END:
:STATUSLOG:
- State "DONE"       from "TODO"       [2022-12-21 Wed 16:56]
:END:
** DONE Multi-Fidelity Learning 
:STATUSLOG:
- State "DONE"       from "TODO"       [2022-12-21 Wed 11:26]
- State "DONE"       from "TODO"       [2022-09-13 Tue 12:51]
:END:
The state of the art in materials modeling favors [[Acrfull:gnn][Graph Neural Network
(GNN)]][[cite:&chen-2019-graph-networ;&choudhary-2021-atomis-line;&xie-2018-cryst-graph]]
architectures. These [[gls:dl][deep learning]] models have sufficient flexibility
to capture the continuous variability in relative positions of
crystals and molecules. They are extremely effective models, but they
are difficult to use with physical materials. Accurately
characterizing structures at a level of atomic granularity cannot be
achieved even with state of the art 3D Electron Tomography
techniques[[cite:&ercius-2015-elect-tomog]]. Yet, characterization of
chemical composition is a well established practice i.e using X-ray
spectroscopy.

Graph convolutional neural networks can power more accurate
structure-target predictions at multiple
fidelities[[cite:&chen-2020-multi-fidel]] by performing [[Gls:mtl][Multi-Task
Learning]] (MTL). For instance, this multiple-fidelity machine learning
technique can infer the relationships between more plentiful [[ACRshort:pbe][PBE]] [[ACRshort:gga][GGA]]
data and rarer but more accurate [[ACRshort:hse][HSE06]] data based on a shared set of
predicting [[gls:ft][features]]. This relationship, if sufficiently general, can
be used to reliably extrapolate from known points on the [[ACRshort:pbe][PBE]] co-domain
to the unknown [[ACRshort:hse][HSE]] co-domain. Of course, while this is implemented
successfully in neural networks, the concept holds for any model
architecture that can simultaneously regress multidimensional targets
which do not need to constitute one rectangular data structure.

Additionally, there are alternative [[gls:mtl][multi-task learning]] approaches
that can be implemented on the domain side. This circumvents the
requirement for flexibility in encoding the co-domain, making it
possible to use a single target regression methods to learn rules for
multiple outcomes that vary depending on a categorical variable
representing the fidelity. In general, the problem of accurately
modeling low availability, high fidelity targets is approached using
MTL to learn regressions on datasets compiled either along X or Y from
measurements taken at multiple [[gls:lot][levels of theory]].

Semi-supervised
learning[[cite:&chapelle-2006-semi-super-learn;&lee-2013-pseud-label]] is
a competing set of methods achieving similar outcomes.

We will employ the domain-side approach where the largest, lowest
fidelity component of our dataset consists of [[acrfull:dft][density functional
theory (DFT)]] band gap predictions made at the [[acrfull:gga][generalized gradient
approximation (GGA)]] [[acrfull:pbe][Perdew-Burke-Ernzerhof (PBE)]] [[gls:lot][level of theory]]. On
the other end, the smallest and highest fidelity subset of the sample
consists of experimental measurements of physical devices collected
from the literature.

** DONE Dataset Overview
:STATUSLOG:
- State "DONE"       from "TODO"       [2022-12-21 Wed 12:16]
:END:
*** DONE Perovskite Band Gaps
:STATUSLOG:
- State "DONE"       from "TODO"       [2022-12-21 Wed 12:16]
:END:
# need to introduce the significance of predicting band gaps more
# effectively
we aim to accurately predict performance-relevant [[Acrfull:hap][Halide perovskite
(HaP)]] band gaps which are strongly predictive of photovoltaic
performance[[cite:&mannodi-kanakkithodi-2019-compr-comput]].

Furthermore, using MTL modeling we aim to predict the experimentally
measured band gaps of compounds that have only been simulated to
date. Our fidelity hierarchy climbs from [[ACRshort:dft][DFT]] simulations performed
using the basic [[acrshort:pbe][PBE]] [[acrshort:gga][GGA]] functional, to results obtained from physical
experiments aggregated in
literature[[cite:&almora-2020-devic-perfor;&kim-2014-cdses-nanow;&swanson-2017-co-sublim]]
see table ref:tbl:LoTs.

While we acknowledge the advantages of [[acrshort:gnn][GNNs]], we aim to express band
gap as functions primarily of the perovskite composition. It is known
this effort is suboptimal especially as the octahedral arrangement of
perovskites is most relevant to their electronic structure,
Nevertheless, a strong understanding of the influence of chemical
composition on performance will continue to be a priority as it is
expected to aid in [[Future Work][inverse design]] and in [[Feature Engineering][Feature Engineering]].

#+begin_src jupyter-python :post wraptbl(*this*, c="label:tbl:LoTs Density Functionals vs Sample Counts")
  my.LoT.value_counts().to_frame()
#+end_src

#+RESULTS:
:results:
 
#+CAPTION: label:tbl:LoTs Density Functionals vs Sample Counts 
|              |  LoT |
|--------------+------|
| PBErel       |  517 |
| HSErel(SOC)  |  299 |
| HSErel       |  297 |
| HSE-PBE(SOC) |  244 |
| EXP          |   44 |
|--------------+------|
| Total        | 1401 |
#+TBLFM: $2=vsum(@I..@II)
:end:

A detailed analysis of this combined hybrid organic-inorganic and
purely inorganic [[acrshort:hap][HaP]] [[ACRshort:dft][DFT]] dataset is covered in a prior article by
[[citet:&yang-2022-high-throug]] and in [[DFT Details]].

Naturally, the statistics obtained from each fidelity vary (fig
ref:fig:bg_dist). This is the primary challenge we will address with
the categorically dimensioned multi-fidelity models discussed in
[[Methods]].

#+begin_src jupyter-python :post wrap(*this*, c="label:fig:bg_dist Variability in band gap per level of theory")
  p = px.histogram(Y, x="bg_eV", color="LoT", barmode="stack")
  p.update_xaxes(title="Band Gap [eV]")
  p.show('svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 300pt
#+CAPTION: label:fig:bg_dist Variability in band gap per level of theory
#+attr_org: :width 700
[[file:./.ob-jupyter/d2e07289c9acdbf81dae438b7d5241ccb00224a8.svg]]
:end:

*** DONE Sampling
:STATUSLOG:
- State "DONE"       from              [2022-09-13 Tue 12:51]
:END:
The simulations for each [[gls:lot][level of theory]] are performed on some number
of members to a fixed subset of the total 37785 compositions that can
be combinatorially generated in a 2x2x2 perovskite supercell when
allowing /at most/ single-site alloying with our 14 constituent
candidates for 3 sites (table ref:tbl:site_tbl).

#+caption: label:tbl:site_tbl ABX_3 Chemical Domain
| /      | <  |    |    |    |    |    |
| A-site | [[ACRshort:ma][MA]] | [[ACRshort:fa][FA]] | Cs | Rb | K  |    |
| B-site | Pb | Sn | Ge | Ba | Sr | Ca |
| X-site | I  | Br | Cl |    |    |    |

Within this sample space, we try to maintain a balance in the share of
samples that represent each one of the "[[gls:cmix][cardinal mixing]]"
categories. Additionally, within each mix we try to maintain a
reasonable balance of purely inorganic samples versus hybrid
organic-inorganic samples. See figure ref:fig:lot_mix_org. See [[Methods]]
for details on how these categories were utilized in model
development.

The design of this dataset provides an opportunity to assess the
ability of our models to extrapolate with respect to alloying scheme
as well as [[gls:lot][level of theory]]. It also provides an opportunity to
investigate the statistical impact of constituent compounds on
perovskite property prediction. See [[Results and Discussion]].

#+begin_src jupyter-python :post wrap(*this*, c="label:fig:lot_mix_org Share by count of total data apportioned from each experimental subcategory")
  p = px.sunburst(my, path=['LoT','mix','org'])
  p.update_layout(
      margin=dict(l=0, r=0, t=0, b=0),
  )
  llist = p.data[0].labels
  vlist = [v if not re.search(r'[0-9]', l) else "" for l,v in zip(p.data[0].labels, p.data[0].values)]
  p.data[0].labels=np.array(list(zip(llist,vlist)))

  p.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 300pt :options inkscapeformat=png, inkscapedpi=300
#+CAPTION: label:fig:lot_mix_org Share by count of total data apportioned from each experimental subcategory
#+attr_org: :width 700
[[file:./.ob-jupyter/714175a3d13e3c523962f378572a20cc21500c4d.svg]]
:end:

** DONE Model Optimization
:STATUSLOG:
- State "DONE"       from "TODO"       [2022-12-21 Wed 16:56]
:END:
# not every scrap of data collected in the experiments was included in the sample
# standard cleaning and deduping operations need only be mentioned

The rigorous [[gls:hp][hyper-parameter]] Optimization (HPO) of any feature
engineering and modeling pipeline is a problem discussed extensively
in the literature. HPO approaches can be broadly separated into
exhaustive and efficient optimization strategies
[[cite:&yang-2020-hyper-optim]]. We use a two-stage procedure for
selecting the best model parameters.

The first stage is an exhaustive grid-search over diversely sampled
parameter space. Each combination of parameters instantiates a model
which is then fit to each of a set of stratified training subsets
generated by a K=3 [[gls:kfs][K-fold split]] [[gls:cv][cross-validation]] strategy. Every
fitted model is subsequently tested against the [[gls:cv][cross-validation]] test
sets and a suite of regression scoring metrics are applied to each
member category simultaneously using a custom scikit-learn score
adapter[fn:4].

The grid search is then narrowed to a high performance quadrant of the
search space by the model evaluator based on recommendations made by a
simple entropy minimization algorithm[fn:4].

# implemented in the "yogi"
# supplementary package under the yogi.model_selection.butler module --
# see documentation for the various grid-narrowing strategies available.

In general, the recommended grid quickly eliminates under-performing
settings based on the sample probability of a setting appearing in a
set of finalists according to the scoring rankings. The selection
score is additionally influenced by a weighted sum of the scoring
ranks allowing for considerably tuning the selection criterion.  For
best results, a few different grid spaces should be explored to
corroborate eliminations.

After the recommendation is made, the granularity of the grid is
increased in the remaining ambiguous parameters and the process is
repeated. In general, no more than 2 or 3 exhaustive searches are
needed over a given set of grids.

Past this point, continuously variable hyper parameters can be
individually optimized using validation curves.

* DONE Methods
:PROPERTIES:
:CLASS: unnumbered
:END:
:STATUSLOG:
- State "DONE"       from "TODO"       [2022-12-21 Wed 16:53]
:END:
** DONE DFT Details
:PROPERTIES:
:CLASS: unnumbered
:END:
:STATUSLOG:
- State "DONE"       from "TODO"       [2022-12-21 Wed 15:31]
:END:
The largest subdivision of 1400 compounds correspond to a series of
optoelectronic [[acrshort:dft][DFT]] simulations. The simulated experiments are
performed on some subset (table ref:tbl:LoTs) of ~500, exclusively
pseudo-cubic, ABX_3 supercells obtained by geometry optimization of
modified structure files [[cite:&pilania-2016-machin-learn]] originally
obtained from the Computational Materials Repository[fn:2]. Each cell
demonstrates an [[acrshort:sqs][SQS]] mixed composition at none or one of each of the A,
B, or X sites.

Each relaxed structure is made in two ways. Once with the [[acrshort:pbe][PBE]] [[acrshort:gga][GGA]]
functional and once with the [[acrshort:hse][HSE06]] functional. Band gaps are obtained
using a static band structure calculation performed at the same and at
higher [[gls:lot][levels of theory]].

The chosen functionals each offer strengths and weaknesses. [[acrshort:pbe][PBE]] is
inexpensive but typically underestimates band gaps. [[acrshort:hse][HSE06]] is orders of
magnitude more expensive and may fail to converge structure
relaxations but tends to be more trustworthy for electronic structure
properties. HSE06 on PBE relaxation attempts to mitigate the
disadvantages of each individually. The use of [[gls:soc][Spin Orbit Coupling]]
helps to better electronic properties simulation in compounds
containing Lead.
# - D3 is performed for improved Van der Waals dispersion forces

** DONE Featurization of Chemistries
:STATUSLOG:
- State "DONE"       from "TODO"       [2022-12-21 Wed 15:39]
:END:
For \alpha total A-site constituents represented in the whole
database, \beta total B-site constituents, and \gamma total X-site
constituents, we provide a Python tool[fn:3] which robustly coverts
the composition string of each data point into a \(\alpha + \beta +
\gamma\) dimensional composition vector. In the case of our dataset
description[[cite:&yang-2022-high-throug]] \(\alpha + \beta + \gamma =
14\). 

#+CAPTION: An example of the cmcl "ft" feature accessor
#+begin_src jupyter-python :exports both
  subset = [883,886]
  df = Y.Formula[subset].to_frame().ft.comp()
  df.index = Y.Formula[subset]
  print(df)
#+end_src

#+RESULTS:
:results:
:                      MA   FA  Pb     I    Br
: Formula                                     
: MA0.7FA0.3PbI3      0.7  0.3   1  3.00   NaN
: MAPb(I0.41Br0.59)3  1.0  NaN   1  1.23  1.77
:end:

This is naturally a sparse, relatively high dimensional
descriptor. With any growth in the composition space it becomes
sparser. This descriptor has been shown to be effective for
interpolating the properties of irregularly mixed large
supercells[[cite:&mannodi-kanakkithodi-2022-data-driven]], However, a
spare descriptor is generally bad for extrapolative
modeling[[cite:&ghiringhelli-2015-big-data]]. When extrapolation is the
aim, continuously distributed, unique, and linearly independent
features are much more reliable[[cite:&lux-2020-inter-spars]]. Our
attempts to provide a domain with these characteristics results in the
following raw feature space.

- 14 sparse composition vectors extracted from chemical formula
  - generated using cmcl[fn:3]
  - see [[Predictor Variables]] 
- 36 dense site-averaged property space
  - computed as a linear combination of composition vectors and
    measured elemental properties[[cite:&mentel-2014]]
  - see [[Predictor Variables]] 
- 5 categorical dimensions one-hot-encoding [[gls:lot][level of theory]].
  - this provides the domain-side categorical axis for [[gls:mtl][multi-task learning]]
  - see table ref:tbl:LoTs

** DONE Machine Learning Algorithms and Parameter Optimization
:STATUSLOG:
- State "DONE"       from "TODO"       [2022-12-21 Wed 16:26]
:END:
We train [[acrshort:rfr][RFR]] and [[acrshort:gpr][GPR]] models of band gap on the union of predictor
[[gls:ft][features]] previously discussed. The [[acrshort:rfr][RFR]] is a flexible nonlinear model,
the [[acrshort:gpr][GPR]] a principled linear model. We hope [[Acrfull:shap][Shapley Additive
Explaination (SHAP)]] analysis of the models will lend insight to the
average physical impacts of 1) site-specific alloying and 2) using
organic molecules in the Perovskite superstructure.

# Our model development will test the hypothesis that formula that fall
# within one of the four mixing categories or one of the two
# hybrid-organic/inorganic categories will share some distributed
# qualities with others in that category.

We are careful to maintain the diversity of mixing types and
hybrid-organic/inorganic samples within each fidelity subset. We
expect this will help to ensure the models learn relationships between
fidelities, not differences in alloy scheme or constituency
distributions within each fidelity.

Each model architecture is rigorously optimized with regard to both 1)
generality over the domains of Perovskite compositions and
site-averaged constituent properties and 2) generality over the domain
of alloy classifications.

In order to monitor for possible categorical biases effecting
regressions, nine metrics are used to evaluate the performance of each
model over all alloy types at every stage of the hyper-parameter
optimization. This is done simultaneously, only models that perform
uniformly well on all alloy types are selected.

We expect perovskites of a given alloy class and of a given
hybrid-organic/inorganic status will perform significantly differently
with respect to a particular application compared to perovskites of a
another class or status. We attempt to make models that reasonably
explain this high entropy mixing diversity by utilizing the [[gls:cmix][cardinal
mixing]] represented in our sample.

We do this by training each model using two test/train splits. First,
the optimal model parameters are chosen for their performance under a
random split. A minimum of 3-fold cross-validation is performed for
every set of model parameters that is considered (See [[Learning
Curves]]). Finally, the optimized model's ability to extrapolate is
tested by training/testing on splits determined with a [[gls:gkf][groupwise
K-fold]] splitting strategy.

Two separate cross validation schemes are employed at each stage of
the design process. First, the sample set is shuffled once and split
to mitigate the models tendency to fit on sample order, then,
stratified K-folds are generated in manner consistent with the types
of each sample. The regressor is then trained on the subsets of each
class. Its ability to extrapolate is independently metered on each
validation fold consisting of members of the other classes.

Second, the ability for a model trained on samples belonging to one
class/status to extrapolate to samples of another class/status is
tested as well. The samples again are shuffled and split. then the
training set is separated using a grouping K-fold split strategy.

A final best model is instantiated using the overall best performing
parameters. These models are finally validated against the test sets
originally split off from the sample in both their extrapolative
ability and consistency across groups.

This procedure in demonstrated via an online Notebook by
[[citet:&manganaris-2022-mrs-comput]] hosted on the Purdue NanoHUB.

** DONE Feature Engineering
:STATUSLOG:
- State "DONE"       from "TODO"       [2022-12-21 Wed 16:51]
:END:
There has been success in creating analytical expressions for
perovskite properties, particularly lattice
parameters[[cite:&jiang-2006-predic-lattic]]. In an attempt to find an
analytical predictor for band gap we employ the [[Acrfull:sisso][Sure Independence
Screening and Sparsifying Operator (SISSO)]][[cite:&ouyang-2018-sisso]].

SIS[fn:1] is a powerful application of compressed
sensing[[cite:&ghiringhelli-2017-learn-physic]]. The SIS operator is a
potent dimensionality reduction technique. It does not perform any
mathematical decomposition but instead picks existent dimensions that
begin to approximate an orthogonal basis it outperforms
CUR[[cite:&ray-2021-various-dimen;&hamm-2019-cur-decom]] decomposition by
functioning effectively in extremely high rank vector spaces. This is
accomplished by posing the decomposition as a compressed sensing
problem in the correlation metric space.

It allows the program to effectively find candidates for a linearly
independent basis in a vector space of immense size. unlike legacy
techniques, e.g. LASSO, it does not suffer when [[gls:ft][features]] are
correlated[[cite:&tibshirani-1996-regres-shrin;&gauraha-2018-introd-to-lasso]]. This
allows for it to be used in performing a brute force search of a
super-space generated by combinatorial operations on the raw [[Predictor
Variables]].

The Sparsifying Operator finds members of the resulting basis set
which correlate with the target co-domain. it does this by creating a
sparsified linear model, similar again to a LASSO. This process
produces an analytic model of the target property, which is easy to
interpret and can even be constrained for consistent combination of
dimension units.

Subsequent applications of the SIS operator to the residuals of this
model are a clever interrogation of error[[cite:&mayo-1998-error-growt]]
yielding more orthogonal basis sets that can be incorporated into the
model

[[acrshort:sisso][SISSO]] is run for our dataset on the same partitioning scheme used by
the previous models. Additionally, the algorithm is informed of
[[gls:ft][features]] units so that it is restricted to meaningful linear
combinations. SIS [[gls:ft][features]] complexity is restricted to a maximum of 3
operations primarily to encourage parsimonious descriptions. The
available operation set is outlined in table ref:tbl:ops.

#+CAPTION: label:tbl:ops operations for formation of combinatorial super-space
| Binary (dimensional) | Unary (dimensionless) |
|----------------------+-----------------------|
| addition             | reciprocation         |
| subtraction          | power 2               |
| multiplication       | power 3               |
| division             | natural logarithm     |
|                      | exponentiation        |
|                      | root 2                |

* TODO Results and Discussion
:PROPERTIES:
:CLASS: unnumbered
:END:
** DONE COMMENT create subplots
:STATUSLOG:
- State "DONE"       from              [2022-12-21 Wed 13:11]
:END:
*** rfr
#+begin_src jupyter-python
  data = pd.read_csv(os.path.expanduser('~/data/perovskites/rfr_pred.csv'), index_col=0)
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python :post wrap(*this*)
  x='true'
  y='pred'
  p1 = px.scatter(
      data[data.partition=='test'],
      x=x, y=y,
      # facet_col='partition',
      hover_name="Formula",
      color="LoT"
  )
  p1.update_traces(
      marker_size=10.
      # marker_opacity=0.2,
      # selector={'marker_symbol':'circle'}
  )
  xlims = min(data[x]), max(data[y])
  ylims = min(data[y]), max(data[y])
  p1.add_scatter(x = [min(xlims+ylims), max(xlims+ylims)],
                y = [min(xlims+ylims), max(xlims+ylims)],
                mode='lines', name="parity", marker=dict(color="black"),
                row='all', col='all')
  p1.update_layout(title_text='Band Gaps [eV] (test partition)',
                  font_family='arial narrow', font_size=30,
                  margin=dict(t=70,b=0,l=0,r=0))
  p1.update_xaxes(title='DFT Calculation', constrain='domain', mirror = True)
  p1.update_yaxes(title='ML Prediction', scaleanchor='x', mirror=True)
  #p1.update_annotations(visible=False)
  p1.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 300pt
#+CAPTION: 
#+attr_org: :width 700
[[file:./.ob-jupyter/68f21ba48c8b938ab0efbd8ee65b3388642d086e.svg]]
:end:

*** gpr
#+begin_src jupyter-python
  data = pd.read_csv(os.path.expanduser('~/data/perovskites/gpr_pred.csv'), index_col=0)
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python :post wrap(*this*)
  x='true'
  y='pred'
  p2 = px.scatter(
      data[data.partition=='test'],
      x=x, y=y,
      # facet_col='partition',
      hover_name="Formula",
      color="LoT"
  )
  p2.update_traces(
      marker_size=10.
      # marker_opacity=0.2,
      # selector={'marker_symbol':'circle'}
  )
  xlims = min(data[x]), max(data[y])
  ylims = min(data[y]), max(data[y])
  p2.add_scatter(x = [min(xlims+ylims), max(xlims+ylims)],
                y = [min(xlims+ylims), max(xlims+ylims)],
                mode='lines', name="parity", marker=dict(color="black"),
                row='all', col='all')
  p2.update_layout(title_text='Band Gaps [eV] (test partition)',
                  font_family='arial narrow', font_size=30,
                  margin=dict(t=70,b=0,l=0,r=0))
  p2.update_xaxes(title='DFT Calculation', constrain='domain', mirror = True)
  p2.update_yaxes(title='ML Prediction', scaleanchor='x', mirror=True)
  #p2.update_annotations(visible=False)
  p2.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 300pt
#+CAPTION: 
#+attr_org: :width 700
[[file:./.ob-jupyter/5f3919f6cd6e7e3b2c299d1361a3b109d05f81cf.svg]]
:end:

** DONE Best Models on Raw Domain
:STATUSLOG:
- State "DONE"       from              [2022-12-21 Wed 13:21]
:END:
# make sure to create subplots first
#+begin_src jupyter-python :post wrap(*this*, w="450pt :float multicol :options inkscapeformat=png, inkscapedpi=300 inkscapeformat=png, inkscapedpi=300", c="label:fig:pairplots Multi-fidelity model predictions vs Experimental values")
    p0 = make_subplots(rows=1, cols=2)

    for trace1, trace2 in zip(p1.data, p2.data):
        p0.add_trace(trace1, row=1, col=1) 
        p0.add_trace(trace2, row=1, col=2)

    import copy

    lo = copy.deepcopy(p0.layout)
    p0.update_layout(p2.layout)
    p0.update_layout(xaxis=p2.layout.xaxis, yaxis=p2.layout.yaxis, overwrite=False)
    p0.update_layout(xaxis2=p1.layout.xaxis, yaxis2=p1.layout.yaxis, coloraxis=p1.layout.coloraxis, overwrite=False)
    p0.layout.xaxis2.scaleanchor=None
    p0.update_layout(lo, overwrite=False)
    # p0.update_layout(showlegend=False)
    p0.update_traces(showlegend=False, row=1, col=1)
    # p1.update_xaxes(constrain='domain', row=1, col=2)
    p0.layout['yaxis2']['scaleanchor'] = 'x2'
    p0.layout['xaxis']['domain'] = [0.0, 0.49]
    p0.layout['xaxis2']['domain'] = [0.51, 1.0]

    p0.update_xaxes(tickvals=list(range(8)))
    p0.update_yaxes(title="RFR Prediction", row=1, col=1)
    p0.update_yaxes(title="GPR Prediction", tickvals=[], title_standoff=0, row=1, col=2)
    p0.update_layout(width=1100)

    p0.update_traces(marker_line=dict(color = 'black', width = 1))

    p0.add_annotation(x=2.5, y=6, xref='x', yref='y',
                      text="r2 = 0.991352<br>maxerr = 0.764664<br>rmse = 0.134671<br>", font_size=20,
                      showarrow=False)

    p0.add_annotation(x=2.5, y=6, xref='x2', yref='y',
                      text="r2 = 0.970821<br>maxerr = 1.993995<br>rmse = 0.258387<br>""", font_size=20,
                      showarrow=False)

    p0.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 450pt :float multicol :options inkscapeformat=png, inkscapedpi=300
#+CAPTION: label:fig:pairplots Multi-fidelity model predictions vs Experimental values
[[file:./.ob-jupyter/e8d56a58c17c3b333f96a8fa83097eab4eca15ee.svg]]
:end:

The exhaustively optimized models are obviously high performing (Table
ref:tbl:rawLoTscores). The [[acrshort:rfr][RFR]] [[gls:hp][hyper-parameters]] are listed in [[HPO
Summary Tables]] (Table ref:tbl:rfrHPO). The [[acrshort:gpr][GPR]] model is tried with
multiple kernels. Ultimately, the best is a non stationary Matern
kernel with \(\nu = \frac{3}{2}\).

** TODO SHAP Analysis of Domain
:STATUSLOG:
- State "DONE"       from "TODO"       [2022-12-21 Wed 22:20]
:END:
#+CAPTION: label:fig:rfrSHAP Random Forest Regression Band Gap SHAP Values
#+attr_latex: :width 450pt
[[file:/home/panos/Documents/manuscripts/DFT+ML+feature_engineering/RFR/.ob-jupyter/704d3204752bc52a936aeb9d96e45380512c8c3d.png]]

[[acrshort:shap][SHAP]] scores are computed automatically for every dimension of every
sample in the domain. The sum of [[acrshort:shap][SHAP]] scores computed for each
predictor variable of a sample is the model's prediction for that
sample[[cite:&lundberg-2017-unified-approac]].

Figures ref:fig:rfrSHAP and ref:fig:gprSHAP show the aggregated score
results. Within each figure, [[gls:ft][features]] are ranked by overall value on
the y-axis. The x-axis shows the [[acrshort:shap][SHAP]] score for each point. The points
are shaped in a violin plot to show the distribution of effects the
presence of the given feature can have. Finally, on the z-axis,
feature value gives a sense of how often the feature is a relatively
minor contributor to a prediction.

For instance, in figure ref:fig:rfrSHAP, the B-site Electronegativity
is fairly often a strongly positive contributor to the [[acrshort:rfr][RFR]]
prediction. However, almost always in this case it is out-contributed
by other [[gls:ft][features]] -- it does not mostly determine the result but it is
still valuable. On the contrary, when it is a strongly negative
contributor it effectively determines the result, the model uses small
positive contributions from other features to claw back up to a
positive valued band gap.

#+CAPTION: label:fig:gprSHAP Gaussian Process Regression Band Gap SHAP Values
#+attr_latex: :width 450pt
[[file:/home/panos/Documents/manuscripts/DFT+ML+feature_engineering/GPR/.ob-jupyter/df8f4cdfbd4884fe978d0b0dc85e13e95137248c.png]]

These [[acrshort:shap][SHAP]] scores can be used to gain insights into the contributions
of site members and site member properties to the perovskite band
gap.

# it is necessary to find which B-elements strongly positive SHAP scores belong to
# next, find which predictors these correspond with
# try to visualize a pattern

** TODO Comparing Feature Usefulness to Raw Correlations
:PROPERTIES:
:ID:       5b5623d8-3db9-44e9-8e94-75644b3ef5b7
:END:
It is interesting to see how models make use of features in light of
basic bi-variate correlations. The only [[gls:ft][features]] that correlate
strongly with band gap are illustrated in figure ref:fig:rpear.

# Likewise, with respect to the generality measure conducted earlier, it
# seems the presence of individual elements is far more predictive of
# the total band gap than mix status. This would explain why X-site and
# A-site alloys are sufficient to predict the band gaps of B-site alloys,
# despite those groups containing no B-site alloys themselves, they do
# contain a representative sample of B-site elements.

#+begin_src jupyter-python
  XY = pd.concat([XX, YY], axis=1).select_dtypes(np.number).fillna(0)
  pearson = pd.DataFrame(np.corrcoef(XY, rowvar=False),
                         columns=XY.columns,
                         index=XY.columns)
  sub = pearson.iloc[56, 0:55]
  ytitles = ["Band Gap [eV]"]
  sub = sub.loc[(sub.abs()>0.5).apply(bool)].to_frame().T
#+end_src

#+begin_src jupyter-python :post wrap(*this*, "200pt :options inkscapeformat=png, inkscapedpi=300", "label:fig:rpear raw features with (\\(|p|>0.5\\)) against band gap")
  p = px.imshow(sub, color_continuous_scale='RdBu_r', zmin=-1, zmax=1,
                labels=dict(color="Pearson Coefficient"),
                y = ytitles, text_auto='.2f',
                height=300
                )
  p.update_xaxes(tickangle=-35)
  p.update_yaxes(tickangle=-35, )
  p.update_layout(coloraxis=dict(colorbar=dict(len=1, orientation='h', y=0.65)))
  p.update_layout(
      paper_bgcolor='rgba(255,255,255,0)',
      plot_bgcolor='rgba(255,255,255,0)',
      margin=dict(t=0,b=0,l=0,r=0),
  )
  p.update_layout(
      xaxis = dict(tickmode='array',
                   tickvals=list(range(sub.columns.shape[0])),
                   ticktext=sub.columns.str.slice(6).str.replace("'", "").str.replace("_"," "),
                   font_size=40
      xaxis = dict(font_size=40)
  ))
  p.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 200pt :options inkscapeformat=png, inkscapedpi=300
#+CAPTION: label:fig:rpear raw features with (\(|p|>0.5\)) against band gap
#+attr_org: :width 700
[[file:./.ob-jupyter/bee19f4a89b8480a3d67cc87af798e711d963581.svg]]
:end:

** TODO SISSO Model and Incidental Engineered Features
The [[Acrfull:sisso][Sure Independence Screening and Sparsifying Operator (SISSO)]]
[[cite:&ouyang-2018-sisso;&ghiringhelli-2017-learn-physic]] is a specific
combination of multiple data mining techniques chained together
resulting in a symbolically expressed regression model.

The best model for band gap returned by 5 trials of SISSO involves 3
SIS [[gls:ft][features]] and has an unremarkable RMSE of 0.4625 eV, barely
outperforming an OLS regression on 55 dimensions
ref:tbl:rawLoTscores. It is expressed in equation
ref:eq:bgexp. Notably, while the units of the expression do not match
the units of band gap as measured (target units are unknown to the
algorithm), they are still energy units.

#+CAPTION: label:eq:bgexp Band gap estimator expression
\begin{align}
bg\mbox{ [eV]} = 1.752075117(&(X;\mbox{Electronegativity}*A;\mbox{Heat of Fusion})\\ \nonumber
                &-(B;\mbox{Ionization Energy}+B;\mbox{Electron affinity}))\\ \nonumber
- 0.5759612116(&B;\ce{Sn}+X;\mbox{Z}-\mbox{HSErel}-\mbox{PBErel})\\ \nonumber
+ 1.074246385(&(A;\mbox{Electronegativity}-B;\ce{Ca})\\ \nonumber
             &\times(B;\mbox{Heat of Vaporization}-X;\mbox{Ionization Energy}))\\ \nonumber
+ 5.254074603 &\mbox{ [kJ/mol]}
\end{align}

#+CAPTION: label:tbl:rawLoTscores RMSE of models on raw domain calculated per LoT subset
| Score Categories  |      GPR |      RFR | Linear OLS | SISSO |
|-------------------+----------+----------+------------+-------|
| rmse              | 0.258387 | 0.134671 |   0.499558 |       |
| rmse_EXP          | 0.157147 | 0.188727 |   0.307186 |       |
| rmse_PBE          | 0.204187 | 0.102778 |   0.472430 |       |
| rmse_HSE          | 0.337971 | 0.170726 |   0.558077 |       |
| rmse_HSE(SOC)     | 0.275642 | 0.102896 |   0.535087 |       |
| rmse_HSE-PBE(SOC) | 0.245003 | 0.162380 |   0.466364 |       |

Computing and combining more than 3 SIS [[gls:ft][features]] is not rewarding of
the computational expense. Residuals are increasingly uncorrelated
with the generated SIS [[gls:ft][features]] and model accuracy gains do not
outstrip complexity. However, in the process of creating Equation
ref:eq:bgexp, 150 SIS predictor variables were determined and
recorded. 50 primary predictors, 50 first residual predictors, and 50
second residual predictors. These can serve as a high quality,
introspective domain for the other architectures to fit on.

We set the aim of decreasing \(\mathcal{O}(n^3)\) computational
expense of [[acrshort:gpr][GPR]] by \approx{}10 times. So, we aim to take 20 highly
correlated [[gls:ft][features]] (slightly less than one half the number used by
prior models) from these SIS subspaces. We expect this to solve the
problems inherent to the raw [[Featurization of Chemistries]]

Computing [[ACRshort:gpr][GPR]] in conjunction with SIS [[gls:ft][features]], we can create
extrapolative models on a much more continuous domain. Using [[ACRshort:gpr][GPR]] on
the SIS subspaces also leverages SISSO's explicability while obtaining
uncertainty estimates, which is helpful for our inverse design
ambitions.

** TODO COMMENT create subplots
*** rfr
#+begin_src jupyter-python
  data = pd.read_csv(os.path.expanduser('~/data/perovskites/rfr_pred.csv'), index_col=0)
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python :post wrap(*this*)
  x='true'
  y='pred'
  p1 = px.scatter(
      data[data.partition=='test'],
      x=x, y=y,
      # facet_col='partition',
      hover_name="Formula",
      color="LoT"
  )
  p1.update_traces(
      marker_size=10.
      # marker_opacity=0.2,
      # selector={'marker_symbol':'circle'}
  )
  xlims = min(data[x]), max(data[y])
  ylims = min(data[y]), max(data[y])
  p1.add_scatter(x = [min(xlims+ylims), max(xlims+ylims)],
                y = [min(xlims+ylims), max(xlims+ylims)],
                mode='lines', name="parity", marker=dict(color="black"),
                row='all', col='all')
  p1.update_layout(title_text='Band Gaps [eV] (test partition)',
                  font_family='arial narrow', font_size=30,
                  margin=dict(t=70,b=0,l=0,r=0))
  p1.update_xaxes(title='DFT Calculation', constrain='domain', mirror = True)
  p1.update_yaxes(title='ML Prediction', scaleanchor='x', mirror=True)
  #p1.update_annotations(visible=False)
  p1.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 300pt
#+CAPTION: 
#+attr_org: :width 700
[[file:./.ob-jupyter/10fa09156cc4b175e28eeac2a6d9a03ae1191db4.svg]]
:end:

*** gpr
#+begin_src jupyter-python
  data = pd.read_csv(os.path.expanduser('~/data/perovskites/gpr_pred.csv'), index_col=0)
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python :post wrap(*this*)
  x='true'
  y='pred'
  p2 = px.scatter(
      data[data.partition=='test'],
      x=x, y=y,
      # facet_col='partition',
      hover_name="Formula",
      color="LoT"
  )
  p2.update_traces(
      marker_size=10.
      # marker_opacity=0.2,
      # selector={'marker_symbol':'circle'}
  )
  xlims = min(data[x]), max(data[y])
  ylims = min(data[y]), max(data[y])
  p2.add_scatter(x = [min(xlims+ylims), max(xlims+ylims)],
                y = [min(xlims+ylims), max(xlims+ylims)],
                mode='lines', name="parity", marker=dict(color="black"),
                row='all', col='all')
  p2.update_layout(title_text='Band Gaps [eV] (test partition)',
                  font_family='arial narrow', font_size=30,
                  margin=dict(t=70,b=0,l=0,r=0))
  p2.update_xaxes(title='DFT Calculation', constrain='domain', mirror = True)
  p2.update_yaxes(title='ML Prediction', scaleanchor='x', mirror=True)
  #p2.update_annotations(visible=False)
  p2.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 300pt
#+CAPTION: 
#+attr_org: :width 700
[[file:./.ob-jupyter/4a818e39a1ca199e69e404bac81f520070c30926.svg]]
:end:

** TODO Best Models on Engineered Domain
** CANCELED COMMENT Genetic algorithm results
:STATUSLOG:
- State "CANCELED"   from              [2022-09-11 Sun 19:28] \\
  GA will have a dedicated paper
:END:
- Fitness score vs generation, stability only.

- Fitness score vs generation, stability + band gap.

- Fitness score vs generation, stability + band gap + SLME.

- Performance of GA with NN/GPR/RFR and with comp/elem/comp+elem.

Validation using new DFT calculations on 10 compositions in 4x4x4
supercell. DFT vs ML properties + band structure + absorption spectra.

* DONE Future Work
:PROPERTIES:
:CLASS: unnumbered
:END:
:STATUSLOG:
- State "DONE"       from "TODO"       [2022-12-21 Wed 14:59]
:END:
** Data Science and Materials Engineering
This has been an exercise in explaining a key performance determining
property of [[Acrfull:hap][Halide perovskites (HaP)]] using only composition
information. Of course, the point of this basic approach is to enable
experimental data to be more easily integrated into predictive models
than is possible with graph-based Convolutional Neural Networks.

Naturally, this invites effort to incorporate much mode experimental
data. Some sources by
[[citet:&jacobsson-2021-open-acces;&briones-2021-accel-lattic]] are prime
for consideration.

Notably, in this work, we assume that all compounds involved in this
model manifest in the same pseudo-cubic phase for
simplicity. Unfortunately, this is not nearly accurate to reality. Our
experience with experimental collaborators indicates effective
application of these models to practical synthesis requires that
realistic phase information is somehow incorporated. Ideally this
information can be encoded in an interoperable way for both
computational and physical experiments.

Finally, these models have iterated on prior composition-only models
of band-gap for inverse
design[[cite:&mannodi-kanakkithodi-2022-data-driven]]. They achieve nearly
50% improvement in accuracy by leveraging more data and, in the case
of models on the engineered domain, higher quality predictor
variables. Using the best performing model and an accompanying model
designed to predict stability in an improved, data-driven perovskites
design framework follows naturally.  In particular, the [[ACRshort:gpr][GPR]] should
drive an active learning approach.

An ongoing goal will be to grow a database of experimentally examined
perovskite photovoltaic prototypes with well defined structures.

** Software Tools
A couple of libraries are in development for easing the aggregation,
accessing, sharing, and analysis of this data. The current database is
packaged in "cmcl" at [[http://github.com/PanayotisManganaris/cmcl]] under
the tag v0.1.5. In this early stage of development, cmcl strives to
provide an "inquisitive" interface to perovskite composition feature
computers in the style of the pandas API. At its current stage, it has
been useful for extracting composition vectors from the formula
strings identifying each compound.

Model development and feature extraction is performed using Python and
SciKit Learn v1.2. A library of model evaluation tools to assist with
exhaustive grid search is being maintained in the "yogi" repository at
[[http://github.com/PanayotisManganaris/yogi]]

* WAIT Acknowledgments
:PROPERTIES:
:CLASS: unnumbered
:END:
:STATUSLOG:
- State "WAIT"       from "TODO"       [2022-12-21 Wed 14:51] \\
  update with Arun
:END:
We acknowledge funding from the US Department of Energy SunShot program
under contract #DOE DEEE005956. Use of the Center for Nanoscale
Materials, an Office of Science user facility, was supported by the U.S.
Department of Energy, Office of Science, Office of Basic Energy
Sciences, under Contract No. DE-AC02-06CH11357. We gratefully
acknowledge the computing resources provided on Bebop, a
high-performance computing cluster operated by the Laboratory Computing
Resource Center at Argonne National Laboratory. This research used
resources of the National Energy Research Scientific Computing Center, a
DOE Office of Science User Facility supported by the Office of Science
of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231.
MYT would like to acknowledge support from the U.S. Department of
Energy, Office of Science, Office of Workforce Development for Teachers
and Scientists (WDTS) under the Science Undergraduate Laboratory
Internship (SULI) program. MJD was was supported by the U. S. Department
of Energy , Office of Basic Energy Sciences, Division of Chemical
Sciences, Geosciences, and Biosciences, under Contract No.
DE-AC02-06CH11357.

** Author Contributions
:PROPERTIES:
:CLASS: unnumbered
:END:
A.M.K. conceived the idea. A.M.K. and J.Y. performed the DFT
computations. P.T.M. Trained ML models. All authors contributed to the
discussion and writing of the manuscript.

** Data Availability
:PROPERTIES:
:CLASS: unnumbered
:END:
DFT data and ML models are available from the corresponding author
upon reasonable request. All documents are tracked in the
https://github.com/PanayotisManganaris/manusciprt--multifidelity-dft-ml.git
online repository

** Additional Information
:PROPERTIES:
:CLASS: unnumbered
:END:
The authors declare no competing financial or non-financial interests.

Correspondence and requests for materials should be addressed to Arun
Mannodi Kanakkithodi (email:amannodi@purdue.edu).

* 
:PROPERTIES:
:CUSTOM_ID: references
:CLASS: unnumbered
:END:
#+LATEX:\pagebreak
bibliographystyle:aipnum4-2
bibliography:~/org/bibliotex/bibliotex.bib
#+LATEX:\printglossaries

* TODO Appendix
:PROPERTIES:
:CLASS: unnumbered
:END:
** DONE Predictor Variables
:STATUSLOG:
- State "DONE"       from "TODO"       [2022-12-21 Wed 14:04]
:END:
#+begin_src jupyter-python
  XXc = XX.iloc[:, 0:14].assign(mix=mix).assign(org=org)
  XXcm = pd.melt(
      XXc, id_vars=["mix", "org"]
  ).replace(0, np.NaN).dropna() #drop zeros
  XXcm = XXcm.rename({"value":"proportion"}, axis=1)
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python :post wrap(p=*this*, w="450pt", c="Normalized Distribution of A-site Constituents")
  p = px.histogram(XXcm[XXcm.variable.str.contains("'A',")], x="proportion", facet_col='variable', facet_col_wrap=3, color="mix",
                   barmode='overlay')
  p.update_layout(legend=dict(itemsizing='constant'))
  p.for_each_annotation(lambda a: a.update(text="".join(a.text[16:-1].split("'"))))
  p.update_xaxes(showline=True, linewidth=2, linecolor='black', ticks='outside', showgrid=False)
  p.update_yaxes(showline=True, linewidth=2, linecolor='black', ticks='outside', showgrid=False)
  p.show(renderer="svg")
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 450pt
#+CAPTION: Normalized Distribution of A-site Constituents
#+attr_org: :width 700
[[file:./.ob-jupyter/c8a78405cb1cda4d5ce76da6352ea746d4ae9f0c.svg]]
:end:

#+begin_src jupyter-python :post wrap(p=*this*, w="450pt", c="Normalized Distribution of B-site Constituents")
  p = px.histogram(XXcm[XXcm.variable.str.contains("'B',")], x="proportion", facet_col='variable', facet_col_wrap=3, color="mix",
                   barmode='overlay')
  p.update_layout(legend=dict(itemsizing='constant'))
  p.for_each_annotation(lambda a: a.update(text="".join(a.text[16:-1].split("'"))))
  p.update_xaxes(showline=True, linewidth=2, linecolor='black', ticks='outside', showgrid=False)
  p.update_yaxes(showline=True, linewidth=2, linecolor='black', ticks='outside', showgrid=False)
  p.show(renderer="svg")
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 450pt
#+CAPTION: Normalized Distribution of B-site Constituents
#+attr_org: :width 700
[[file:./.ob-jupyter/5026578029e767c7954e424ba9580ae9c7672b60.svg]]
:end:

#+begin_src jupyter-python :post wrap(p=*this*, w="450pt", c="Normalized Distribution of X-site Constituents")
  p = px.histogram(XXcm[XXcm.variable.str.contains("'X',")], x="proportion", facet_col='variable', facet_col_wrap=7, color="mix",
                   #category_orders={'variable':["Ba", "Ge", "Cl", "Br", "I", "Sn", "Pb", "Cs", "FA", "MA", "Sr", "Ca", "Rb", "K"]}
                   barmode='overlay', height=300)
  p.update_layout(legend=dict(itemsizing='constant'))
  p.for_each_annotation(lambda a: a.update(text="".join(a.text[16:-1].split("'"))))
  p.update_xaxes(showline=True, linewidth=2, linecolor='black', ticks='outside', showgrid=False)
  p.update_yaxes(showline=True, linewidth=2, linecolor='black', ticks='outside', showgrid=False)
  p.show(renderer="svg")
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 450pt
#+CAPTION: Normalized Distribution of X-site Constituents
#+attr_org: :width 700
[[file:./.ob-jupyter/2407b4c5d1b25479ab1ba73dee37ae53f46fd8e2.svg]]
:end:

Site-averaged elemental properties are derived from the composition
vectors. Twelve properties are computed per \(\ch{ABX_3}\)
constituent. These are much denser distributions.
1. Ionic Radius
2. Boiling Temperature
3. Melting Temperature
4. Density
5. Atomic Weight
6. Electron Affinity
7. Ionization Energy
8. Heat of Fusion
9. Heat of Vaporization
10. Electronegativity
11. Atomic Number
12. Period

# Generate by relational database 3-way join with composition vectors as weights

#+begin_src jupyter-python
  XXp = XX.iloc[:, 14:-5].assign(mix=mix).assign(org=org)
  XXpm = pd.melt(
      XXp, id_vars=["mix", "org"]
  ).replace(0, np.NaN).dropna() #drop zeros
  XXpm = XXpm.rename({"value":"proportion"}, axis=1)
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python :post wrap(p=*this*, w="450pt", c="Distributions of Mean A-Site Properties")
  p = px.histogram(XXpm[XXpm.variable.str.contains("'A',")], x="proportion", facet_col='variable', facet_col_wrap=3, color="mix",
                   barmode='overlay')
  p.update_layout(legend=dict(itemsizing='constant'))
  p.for_each_annotation(lambda a: a.update(text="".join(a.text[16:-1].split("'"))))
  p.update_xaxes(showline=True, linewidth=2, linecolor='black', ticks='outside', showgrid=False)
  p.update_yaxes(showline=True, linewidth=2, linecolor='black', ticks='outside', showgrid=False)
  p.show(renderer="svg")
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 450pt
#+CAPTION: Distributions of Mean A-Site Properties
#+attr_org: :width 700
[[file:./.ob-jupyter/693a93309ca0810b0ef94d8b01d1dea58c2b18c5.svg]]
:end:

#+begin_src jupyter-python :post wrap(p=*this*, w="450pt", c="Distributions of Mean B-Site Properties")
  p = px.histogram(XXpm[XXpm.variable.str.contains("'B',")], x="proportion", facet_col='variable', facet_col_wrap=3, color="mix",
                   barmode='overlay')
  p.update_layout(legend=dict(itemsizing='constant'))
  p.for_each_annotation(lambda a: a.update(text="".join(a.text[16:-1].split("'"))))
  p.update_xaxes(showline=True, linewidth=2, linecolor='black', ticks='outside', showgrid=False)
  p.update_yaxes(showline=True, linewidth=2, linecolor='black', ticks='outside', showgrid=False)
  p.show(renderer="svg")
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 450pt
#+CAPTION: Distributions of Mean B-Site Properties
#+attr_org: :width 700
[[file:./.ob-jupyter/a4faa833409744e179146795cdd3b0f60f5c3db2.svg]]
:end:

#+begin_src jupyter-python :post wrap(p=*this*, w="450pt", c="Distributions of Mean X-Site Properties")
  p = px.histogram(XXpm[XXpm.variable.str.contains("'X',")], x="proportion", facet_col='variable', facet_col_wrap=3, color="mix",
                   barmode='overlay')
  p.update_layout(legend=dict(itemsizing='constant'))
  p.for_each_annotation(lambda a: a.update(text="".join(a.text[16:-1].split("'"))))
  p.update_xaxes(showline=True, linewidth=2, linecolor='black', ticks='outside', showgrid=False)
  p.update_yaxes(showline=True, linewidth=2, linecolor='black', ticks='outside', showgrid=False)
  p.show(renderer="svg")
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 450pt
#+CAPTION: Distributions of Mean X-Site Properties
#+attr_org: :width 700
[[file:./.ob-jupyter/c46d84697e84e3f3a88d840460eb128428d81cc3.svg]]
:end:

** TODO Learning Curves
Learning curves are computed for each scorer. Notice that the error
metrics are negated for consistency with the R^2 and ev scores; the
greater the number, the better the model performs.

Cross-validation within the training set is the only way of checking
the generality of models during the grid search. Identifying the
validation split size is necessary to obtain an understanding of how
much data is needed to train a model that can generalize.

More data offers better chances. However, the smaller the split, the
longer and more expensive the loop training becomes, e.g. 10-fold
splits makes for 10 sample scores at each partition size. Meaning, 90%
of the training set is used for actual training and the remaining 10%
is used for validation and this is repeated 10 times.

Shuffling is performed prior to generating each fold. The shuffle is
seeded with a deterministic random state to ensure scores are
comparable across partition size

** TODO COMMENT Generality Tests
** TODO HPO Summary Tables
*** Random Forest Pipeline Control Parameters

#+CAPTION: label:tbl:rfrHPO Select hyper-parameters from exhaustive search of 10368 models
|                          | Search Space                             | Selected Space   |
|--------------------------+------------------------------------------+------------------|
| normalizer__norm         | [l1, l2, max]                            | [l2]             |
| bootstrap                | [True]                                   | [True]           |
| ccp_alpha                | [0.0, 0.002]                             | [0.0]            |
| criterion                | [squared_error, absolute_error, poisson] | [absolute_error] |
| max_depth                | [25, 20]                                 | [20]             |
| max_features             | [auto, 3, 5]                             | [1.0]            |
| max_leaf_nodes           | [700, 800]                               | [750]            |
| max_samples              | [0.9, 0.6, 0.3]                          | [0.9]            |
| min_impurity_decrease    | [0.0, 0.3]                               | [0.0]            |
| min_samples_leaf         | [1]                                      | [1]              |
| min_samples_split        | [2, 5]                                   | [2]              |
| min_weight_fraction_leaf | [0.0]                                    | [0.0]            |
| n_estimators             | [20, 50, 100]                            | [150]            |
| n_jobs                   | [4]                                      | [4]              |
| oob_score                | [True]                                   | [True]           |
| random_state             | [None]                                   | [None]           |
| verbose                  | [0]                                      | [0]              |
| warm_start               | [False]                                  | [False]          |

*** GPR

* Footnotes
[fn:4]https://github.com/PanayotisManganaris/yogi 
[fn:3]https://github.com/PanayotisManganaris/cmcl
[fn:2]https://cmr.fysik.dtu.dk/ 
[fn:1]https://github.com/rouyang2017/SISSO 

#+NAME: wrap
#+begin_src bash :var p="" :var w="300pt" :var c=""
  echo -ne "$p \n#+attr_latex: :width $w\n#+CAPTION: $c"
#+end_src

#+NAME: wraptbl
#+begin_src bash :var p="" :var w="300pt" :var c=""
  echo -ne "$p \n#+CAPTION: $c "
#+end_src

#+name: glossary
| label | term                    | definition                                                                                                                                                                                                                              |
|-------+-------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| cmix  | cardinal mixing         | Describes perovskite alloys where no more than one of the A, B, or X sites is occupied by multiple possible constituents                                                                                                                |
| prtn  | partition               | Portion of sample data reserved for a purpose in model development                                                                                                                                                                      |
| cv    | cross-validation        | Method for gathering statistics on the abilities of a model to fit to the parent partition                                                                                                                                              |
| kfs   | K-fold split            | Data partition divided into K arbitrary groups for use in cross-validation schemes                                                                                                                                                      |
| gkf   | groupwise K-fold        | Data partition divided into K-folds where each fold corresponds to a category label                                                                                                                                                     |
| lot   | level of theory         | Refers to the rank of a [[ACRshort:dft][DFT]] functional in the hierarchy of phenomenological comprehensiveness. A proxy for accuracy.                                                                                                                    |
| mp    | Materials Project       | US Government-led multidisciplinary collaboration founded in 2011 as the Materials Genome Initiative.                                                                                                                                   |
| ml    | machine learning        | a science concerned with algorithms which improve their performance with exposure to new data                                                                                                                                           |
| ft    | features                | attributes of an observed event or object which might empirically explain the event or object                                                                                                                                           |
| hp    | hyper-parameter         | a setting that controls how a learning algorithm works                                                                                                                                                                                  |
| cl    | classical learning      | a paradigm of machine learning that is dependent on expert knowledge to extract quality features from samples in a dataset                                                                                                              |
| sm    | surrogate model         | a representation which attempts to capture as much of the relationship between a domain and a target property as possible                                                                                                               |
| dl    | deep learning           | a paradigm of machine learning differing from classical learning in that the features of the input data are themselves learned by the algorithm                                                                                         |
| ls    | latent space            | a multidimensional abstraction of a problem space. the relationship between coordinates in this space and observation in the real world can be formulated to guarantee the viability of solutions in the abstraction                    |
| eva   | evolutionary algorithms | a class of nature-inspire algorithms often applied to optimization in high dimensional discontinuous functions                                                                                                                          |
| agn   | ALIGNN                  | the Atomistic Line Graph Neural Network considers relative positions of atoms in a crystal as well as the relative angles between bonds by creating two related node and edge graphs and convluting them in a staggered manner together |
| gp    | Gaussian Process        | Any function which returns samples from an underlying multivariate normal distribution                                                                                                                                                  |
| fair  | FAIR                    | Findable Accessible Interoperable and Reusable Data                                                                                                                                                                                     |
| mtl   | multi-task learning     | A type of machine learning where an algorithm learns multiple functions simultaneously, while exploiting commonalities and differences between the functions                                                                            |
| soc   | Spin Orbit Coupling     | An additional term intended to account for the increased relevance of quantum angular momentum to electromagnetic response in heavy atoms                                                                                               |

#+name: acronyms
| label | abbreviation | full form                                            |
|-------+--------------+------------------------------------------------------|
| hap   | HaP          | halide perovskite                                    |
| vasp  | VASP         | Vienna Ab initio Simulation Package                  |
| qmml  | QM/ML        | quantum mechanics machine learning                   |
| slme  | SLME         | spectroscopic limited maximum efficiency             |
| pce   | PCE          | power conversion efficiency                          |
| dft   | DFT          | density functional theory                            |
| gga   | GGA          | generalized gradient approximation                   |
| pbe   | PBE          | Perdew-Burke-Ernzerhof Functional                    |
| hse   | HSE06        | Heyd-Scuseria-Ernzerhof Functional                   |
| ma    | MA           | Methylammonium                                       |
| fa    | FA           | Formamidinium                                        |
| hap   | HaP          | halide perovskite                                    |
| pca   | PCA          | principal component analysis                         |
| tsne  | t-SNE        | t-distributed stochastic neighbor embedding          |
| umap  | UMAP         | uniform manifold approximation and projection        |
| gpr   | GPR          | Gaussian Process Regression                          |
| rfr   | RFR          | Random Forest Regression                             |
| sisso | SISSO        | Sure Independence Screening and Sparsifying Operator |
| sqs   | SQS          | special quasi-random structures                      |
| paw   | PAW          | projector augmented wave                             |
| nist  | NIST         | National Institute of Standards and Technology       |
| pes   | PES          | Potential Energy Surface                             |
| shap  | SHAP         | Shapley Additive Explaination                        |
| gnn   | GNN          | Graph Neural Networks                                |


