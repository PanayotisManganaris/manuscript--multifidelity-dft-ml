* Introduction
  :PROPERTIES:
  :CUSTOM_ID: introduction
  :CLASS: unnumbered
  :END:
...\\

* RESULTS AND DISCUSSION
  :PROPERTIES:
  :CUSTOM_ID: results-and-discussion
  :CLASS: unnumbered
  :END:
*Figure1:*\\
Outline of work: HT-DFT Data Generation \(\rightarrow\) Descriptors and
correlations \(\rightarrow\) Surrogate ML Models (NN, RFR, GPR)
\(\rightarrow\) Inverse design using Genetic Algorithm \(\rightarrow\)
Validation of promising compounds.\\
*Figure2:*\\
Quick visualization of PBE data + correlations with descriptors
(reproduced from other paper?).\\
*Figure3:*\\
All surrogate ML model results:

- RMSE vs training set size for NN, RFR and GPR, using composition only,
  elemental only, and both together, for 3 properties: *decomposition
  energy, band gap, SLME*.

- Best RMSE values for each property for different ML techniques and
  different descriptors

- Parity plots for best ML models for each property

\\
*Figure4:*\\
Genetic algorithm results:

- Fitness score vs generation, stability only.

- Fitness score vs generation, stability + band gap.

- Fitness score vs generation, stability + band gap + SLME.

- Performance of GA with NN/GPR/RFR and with comp/elem/comp+elem.\\

*Figure5:*\\
Validation using new DFT calculations on 10 compositions in 4x4x4
supercell. DFT vs ML properties + band structure + absorption spectra.\\

** ...
   :PROPERTIES:
   :CUSTOM_ID: section
   :CLASS: unnumbered
   :END:
** ...
   :PROPERTIES:
   :CUSTOM_ID: section-1
   :CLASS: unnumbered
   :END:
** Summary
   :PROPERTIES:
   :CUSTOM_ID: summary
   :CLASS: unnumbered
   :END:
...\\

* METHODS
  :PROPERTIES:
  :CUSTOM_ID: methods
  :CLASS: unnumbered
  :END:
** DFT Details
   :PROPERTIES:
   :CUSTOM_ID: dft-details
   :CLASS: unnumbered
   :END:
...\\

** Surrogate ML Models
   :PROPERTIES:
   :CUSTOM_ID: surrogate-ml-models
   :CLASS: unnumbered
   :END:
*** Model Optimization
The rigorous hyper-Parameter Optimization (HPO) of any feature
engineering and modeling pipeline is a problem discussed extensively
in the literature. HPO approaches can be broadly separated into
exhaustive and efficient optimization strategies
[[cite:&yang-2020-hyper-optim]]. We use a two-stage procedure for
selecting the best model parameters.

The first stage is an exhaustive grid-search over diversely sampled
parameter space. Each combination of parameters instantiates a model
which is then fit to each of a set of stratified training subsets
generated by a 3-fold cross-validation strategy. Every fitted model is
subsequently tested against the cross-validation test sets and a suite
of regression scoring metrics are applied simultaneously.

The scoring metrics we choose vary by model architecture. See summary tables.

The grid search is then narrowed to a high performance quadrant of the
search space by the model evaluator based on recommendations made by a
simple entropy minimization algorithm implemented in the "yogi"
supplementary package under the yogi.model_selection.butler module --
see documentation for the various grid-narrowing strategies available.

In general, the recommended grid quickly eliminates under-performing
settings based on the sample probability of a setting appearing in a
set of finalists according to the scoring rankings. The selection
score is additionally influenced by a weighted sum of the scoring
ranks allowing for considerably tuning of the selection criterion.
For best results, a few different grid spaces should be explored to
corroborate eliminations.

After the recommendation is made, the granularity of the grid is
increased in the remaining ambiguous parameters and the process is
repeated.

Additionally/Alternatively,

** Genetic Algorithm
   :PROPERTIES:
   :CUSTOM_ID: genetic-algorithm
   :CLASS: unnumbered
   :END:
...\\

* ACKNOWLEDGMENTS
  :PROPERTIES:
  :CUSTOM_ID: acknowledgments
  :CLASS: unnumbered
  :END:

We acknowledge funding from the US Department of Energy SunShot program
under contract #DOE DEEE005956. Use of the Center for Nanoscale
Materials, an Office of Science user facility, was supported by the U.S.
Department of Energy, Office of Science, Office of Basic Energy
Sciences, under Contract No. DE-AC02-06CH11357. We gratefully
acknowledge the computing resources provided on Bebop, a
high-performance computing cluster operated by the Laboratory Computing
Resource Center at Argonne National Laboratory. This research used
resources of the National Energy Research Scientific Computing Center, a
DOE Office of Science User Facility supported by the Office of Science
of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231.
MYT would like to acknowledge support from the U.S. Department of
Energy, Office of Science, Office of Workforce Development for Teachers
and Scientists (WDTS) under the Science Undergraduate Laboratory
Internship (SULI) program. MJD was was supported by the U. S. Department
of Energy , Office of Basic Energy Sciences, Division of Chemical
Sciences, Geosciences, and Biosciences, under Contract No.
DE-AC02-06CH11357.

** Author Contributions
   :PROPERTIES:
   :CUSTOM_ID: author-contributions
   :CLASS: unnumbered
   :END:
M.K.Y.C., R.F.K. and A.M.K. conceived the idea. A.M.K., M.Y.T. and
F.G.S. performed the DFT computations. A.M.K. and M.J.D. trained ML
models. All authors contributed to the discussion and writing of the
manuscript.

** Data Availability
   :PROPERTIES:
   :CUSTOM_ID: data-availability
   :CLASS: unnumbered
   :END:
DFT data and ML models are available from the corresponding author upon
reasonable request.

** Additional Information
   :PROPERTIES:
   :CUSTOM_ID: additional-information
   :CLASS: unnumbered
   :END:
The authors declare no competing financial or non-financial interests.

Correspondence and requests for materials should be addressed to A.M.K.
(email:amannodi@purdue.edu).

* REFERENCES
  :PROPERTIES:
  :CUSTOM_ID: references
  :CLASS: unnumbered
  :END:
bibliographystyle:authordate1
bibliography:~/org/bibliotex/bibliotex.bib
