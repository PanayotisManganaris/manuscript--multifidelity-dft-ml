% Created 2022-04-12 Tue 14:53
% Intended LaTeX compiler: pdflatex
\documentclass[aip, jmp, amsmath, amssymb, reprint]{revtex4-2}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}

\usepackage{abstract} %% easy abstract environment (from frontmatter pkg "ltxfront")
\usepackage[export]{adjustbox} %% expanded control over image, minipages, etc
\usepackage{amsthm} %% formal mathematics environments
\usepackage{amsfonts} %% formal math fonts
\usepackage{mathptmx} %% ghostscript/postscript fonts and font loading options
\usepackage{bm} %% bold math
\usepackage{caption} %% full (expands on capt-of) control over appearance of float captions
%\captionsetup{margin=10pt, font=small, labelfont=bf} %% (global preamble, local env)
\usepackage{sidecap} %% control of figure and caption positioning and margin spill
\usepackage{mathtools} %% bugfixing and additional tools for amsmath
\usepackage{upgreek} %% easy lower and uppercase nonitalicized greek letters
\usepackage{soul} %% spaceout and underline macros
\usepackage{xcolor} %% text color macros
%\usepackage{natbib} %% required citation engine

%% control margin configurations and visualize framing
%\usepackage[showframe, %%Uncomment any one of the following lines to test
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall, %% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}

%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines
\author{Panayotis Manganaris}
\date{\today}
\title{Combining High-Throughput Computations, Surrogate Models, and Genetic Algorithms for Discovering Novel Halide Perovskites}
\hypersetup{
 pdfauthor={Panayotis Manganaris},
 pdftitle={Combining High-Throughput Computations, Surrogate Models, and Genetic Algorithms for Discovering Novel Halide Perovskites},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.0.50 (Org mode 9.5.2)}, 
 pdflang={English}}
\begin{document}

%revtex frontmatter conventions for quick recall via INCLUDE keyword
\title{%
Combining High-Throughput Computations, Surrogate Models, and\\
Genetic Algorithms for Discovering Novel Halide Perovskites
}
%\thanks{A footnote to the article title}

\author{Panayotis Manganaris}
\author{Jiaqi Yang}
\author{Arun \surname{Mannodi Kanakkithodi}}
\email{amannodi@purdue.edu}
\affiliation{%
School of Materials Engineering,\\
Purdue University, West Lafayette, Indiana 47907, USA
}
\collaboration{Mannodi Research Group}

\date{\today}
%INCLUDE before abstract environment
\begin{abstract}
The ability to predict the likelihood of impurity incorporation and their electronic energy levels in
semiconductors is crucial for controlling its conductivity, and thus the semiconductor's performance in solar
cells, photodiodes, and optoelectronics. The difficulty and expense of experimental and computational
determination of impurity levels makes a data-driven machine learning approach appropriate. In this work, we
show that a density functional theory-generated dataset of impurities in Cd-based chalcogenides CdTe, CdSe,
and CdS can lead to accurate and generalizable predictive models of defect properties. By converting any
\textit{semiconductor + impurity} system into a set of numerical descriptors, regression models are developed
for the impurity formation enthalpy and charge transition levels. These regression models can subsequently
predict impurity properties in mixed anion CdX compounds (where X is a combination of Te, Se and S) fairly
accurately, proving that although trained only on the end points, they are applicable to intermediate
compositions. We make machine-learned predictions of the Fermi-level-dependent formation energies of hundreds
of possible impurities in 5 chalcogenide compounds, and we suggest a list of impurities which can shift the
equilibrium Fermi level in the semiconductor as determined by the dominant intrinsic defects. These
`dominating' impurities as predicted by machine learning compare well with DFT predictions, revealing the
power of machine-learned models in the quick screening of impurities likely to affect the optoelectronic
behavior of semiconductors.
\end{abstract}
\section*{Introduction}
\label{sec:org7aa94ab}
\subsection*{Materials Design Pipeline}
\label{sec:orgbba048d}
\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{Figure1.pdf}
\caption{\label{fig:outline} \textbf{Basic outline, structure and properties.} (a) General outline of materials design process leading to ML-driven prediction of properties based on DFT data and intermediate step of converting materials to numerical descriptors. (b) The Zinc Blende structure adopted by CdTe, CdSe, and CdS. Cd atoms are shown in blue and Te/Se/S atoms in red. The unit cell has been indicated with dashed lines. (c) Comparison of band gaps computed at the PBE and HSE06 levels of theory with reported experimental values \cite{kim-2014-cdses-nanow,swanson-2017-co-sublim}, for CdTe, CdSe, CdS, CdTe\textsubscript{0.5}Se\textsubscript{0.5} and CdSe\textsubscript{0.5}S\textsubscript{0.5}. (d) Outline of the DFT and ML driven prediction of properties of impurities in Cd-based chalcogenides.}
\end{figure}
\subsection*{Dataset Overview}
\label{sec:org0ef901b}
\subsubsection*{Quick visualization of PBE data + correlations with descriptors}
\label{sec:org53abaa5}
(reproduced from other paper?).\\
\subsubsection*{Distribution Analysis}
\label{sec:orged16ac5}
The design of this dataset provides an opportunity to assess the
generalizability of Perovskite property models with respect to alloy
group. It also provides an opportunity to investigate the statistical
impact of constituent compounds on Perovskite property performance.

This will be done by performing model optimization on random
cross-validation folds as well as on 

\begin{itemize}
\item PBE Data
\label{sec:orgef3b33c}
\href{Constituent\_Representations.png}{\begin{figure}[htbp]

\includegraphics[width=.9\linewidth]{/home/panos/MannodiGroup/publications/61b23d041c2d41de2144462b/Dataset_Overview/PBE_Constituent_Representations.png}
\caption{\label{fig:comp} Proportion of constituents present in dataset}
\end{figure}}

\href{Alloy\_Representations.png}{\begin{figure}[htbp]

\includegraphics[width=.9\linewidth]{/home/panos/MannodiGroup/publications/61b23d041c2d41de2144462b/Dataset_Overview/PBE_Alloy_Representations.png}
\caption{\label{fig:group} Share of total data apportioned to each alloy group}
\end{figure}}

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{/home/panos/MannodiGroup/publications/61b23d041c2d41de2144462b/Dataset_Overview/PBE_Constituent_Representations_per_Scheme.png}
\caption{\label{fig:groupcomp} Proportion of constituents present in each alloy group}
\end{figure}

\item HSE Data
\label{sec:org84487b1}
\href{Constituent\_Representations.png}{\begin{figure}[htbp]

\includegraphics[width=.9\linewidth]{/home/panos/MannodiGroup/publications/61b23d041c2d41de2144462b/Dataset_Overview/HSE_Constituent_Representations.png}
\caption{\label{fig:comp} Proportion of constituents present in dataset}
\end{figure}}

\href{Alloy\_Representations.png}{\begin{figure}[htbp]

\includegraphics[width=.9\linewidth]{/home/panos/MannodiGroup/publications/61b23d041c2d41de2144462b/Dataset_Overview/HSE_Alloy_Representations.png}
\caption{\label{fig:group} Share of total data apportioned to each alloy group}
\end{figure}}

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{/home/panos/MannodiGroup/publications/61b23d041c2d41de2144462b/Dataset_Overview/HSE_Constituent_Representations_per_Scheme.png}
\caption{\label{fig:groupcomp} Proportion of constituents present in each alloy group}
\end{figure}
\end{itemize}

\subsubsection*{Univariate Analysis}
\label{sec:org63d61d2}
The computational properties of interest are Perovskite Bandgap,
Decomposition Energy (stability), and SpeSLME. appear to follow an
approximately normal distribution Bandgap and

\subsection*{Software Tools}
\label{sec:orgcb95a87}
A couple of libraries are in development for easing the aggregation,
accessing, sharing, and analysis of this data.  The database itself is
packaged with "cmcl" at \url{http://github.com/PanayotisManganaris/cmcl}.
In this early stage of development, cmcl strives to provide an
"inquisitive" interface to comprehensive chemical data feature
computers in the style of the pandas API. At its current stage, it has
been useful for extracting composition vectors from the formula
strings identifying each compound we explored.

Model development is performed using the intel distribution of python
and SciKit Learn v0.24 and a standard distribution of Tensorflow both
obtained from the anaconda repositories. A library of supplementary
model evaluation tools (designed with particular attention to
smoothing the use of Pandas in ML workflows) is being maintained in
the "yogi" repository at \url{http://github.com/PanayotisManganaris/yogi}

\section*{RESULTS AND DISCUSSION}
\label{sec:orgadd1d0b}
\textbf{Figure1:}\\
Outline of work: HT-DFT Data Generation \(\rightarrow\) Descriptors and
correlations \(\rightarrow\) Surrogate ML Models (NN, RFR, GPR)
\(\rightarrow\) Inverse design using Genetic Algorithm \(\rightarrow\)
Validation of promising compounds.\\


\subsection*{All surrogate ML model results}
\label{sec:orgaa22daa}

\begin{itemize}
\item RMSE vs training set size for NN, RFR and GPR, using composition only,
elemental only, and both together, for 3 properties: \textbf{decomposition
energy, band gap, SLME}.

\item Best RMSE values for each property for different ML techniques and
different descriptors

\item Parity plots for best ML models for each property
\end{itemize}

\subsection*{Genetic algorithm results}
\label{sec:orga5cf203}

\begin{itemize}
\item Fitness score vs generation, stability only.

\item Fitness score vs generation, stability + band gap.

\item Fitness score vs generation, stability + band gap + SLME.

\item Performance of GA with NN/GPR/RFR and with comp/elem/comp+elem.\\
\end{itemize}

\textbf{Figure5:}\\
Validation using new DFT calculations on 10 compositions in 4x4x4
supercell. DFT vs ML properties + band structure + absorption spectra.\\

\subsection*{\ldots{}}
\label{sec:org7f79f3e}
\begin{center}
\begin{tabular}{llrrr}
\textbf{Dataset} & \textbf{Regression} & \textbf{Site Properties} & \textbf{Defect Properties} & \textbf{Site+Defect}\\
\hline
 &  &  &  & \\
 & RFR & 0.40 & 0.20 & 0.17\\
Training & KRR & 0.40 & 0.30 & 0.20\\
 & LASSO & 0.62 & 0.50 & 0.44\\
 &  &  &  & \\
 &  &  &  & \\
 & RFR & 0.65 & 0.45 & 0.38\\
Test & KRR & 0.68 & 0.40 & 0.32\\
 & LASSO & 0.75 & 0.52 & 0.47\\
 &  &  &  & \\
 &  &  &  & \\
 & RFR & 0.84 & 0.57 & 0.52\\
CdTe\(_{0.5}\)Se\(_{0.5}\) & KRR & 0.80 & 0.65 & 0.57\\
 & LASSO & 0.95 & 0.73 & 0.65\\
 &  &  &  & \\
 &  &  &  & \\
 & RFR & 0.86 & 0.63 & 0.57\\
CdSe\(_{0.5}\)S\(_{0.5}\) & KRR & 0.75 & 0.68 & 0.70\\
 & LASSO & 0.92 & 0.70 & 0.72\\
 &  &  &  & \\
\end{tabular}
\end{center}

\subsection*{Summary}
\label{sec:org7ee5663}
\ldots{}\\

\section*{METHODS}
\label{sec:orgc708fdb}
\subsection*{DFT Details}
\label{sec:orge933e0a}
\ldots{}\\

\subsubsection*{{\bfseries\sffamily TODO} Chemical Featurization}
\label{sec:orgef0a217}

\subsection*{Surrogate Modeling Perovskite Properties}
\label{sec:orge3d670f}
\subsubsection*{Objectives}
\label{sec:orgd00d14b}
Our objectives are two fold:

First, we aim to accurately predict performance-relevant Perovskite
band gaps, decomposition energies, and Spectroscopically Limited Maximum
Efficiency (SLME).

We will follow a multi-fidelity approach, where the bulk of affordable
low level-of-theory data will inform and improve the extrapolative
ability of models trained on higher fidelity measurements.

Our fidelity hierarchy climbs from results obtained using a PBE
functional, to results obtained using an HSE functional, to
experimental results aggregated in literature \cite{almora-2020-devic-perfor}.

We aim to express these variables as functions of the Perovskite
composition. Schemes for incorporating structural information will be
developed in future work. Nevertheless, a strong understanding of the
influence of chemical composition on performance will continue to be a
priority as it is expected to aid in screening the combinatorial
chemical space for viable compounds based on both available precursors
and performance criteria.

Furthermore, chemical data can already be mined for a rich feature set
as previously discussed.

Second, we hope to better understand the average physical impacts
of 1) site-specific alloying and 2) using organic molecules in the
Perovskite superstructure.

These goals are not entirely separate from the first goal of
expressing various properties as functions of composition, but they
can be more simply approached as problems of addressing dependencies
in the data statistics. Our model development will test the hypothesis
that formula that fall within one of these classifications will share
some distributed qualities with others that fit their classification.

These are certainly not the only groups of dependent samples
potentially generated by these experiments, but they are the most
noticeable in the structure of the multi-fidelity sample set as we
have constructed it.
\subsubsection*{Considerations}
\label{sec:orgce2260d}
We expect that perovskites of a given alloy class and of a given
hybrid-organic/inorganic status will perform significantly
differently, in all respects, to Perovskites of a another class or
status. By leveraging the even distribution of constituent compounds
in this sample set of Halide Perovskites, we can cast this problem as
a series of cross-validations for independently trained models.

A minimum of 3-fold cross-validation is performed for every set of
model parameters that is considered. 

Two separate cross validation schemes are employed at each stage of
the design process.

First, the sample set is shuffled once and split to mitigate the
models tendency to fit on sample order, then, stratified K-folds are
generated in manner consistent with the classification of each
sample. However, this fold is not used in a classification problem,
the regressor is trained on the subsets of each class, and it's
ability to extrapolate is independently metered on each validation
fold consisting of members of the other classes.

Second, the ability for a model trained on samples belonging to one
class/status to extrapolate to samples of another class/status is
tested as well. The samples again are shuffled and split. then the
training set is separated using a grouping K-fold split strategy (stratified? shuffled?)

Per architecture, a model is instantiated using the, in aggregate,
best performing parameters. These models are finally validated against
the test sets originally split off from the sample in \textbf{both their
extrapolative ability and consistency across groups.}

\subsubsection*{Model Optimization Details}
\label{sec:org3f5ec23}
The rigorous hyper-Parameter Optimization (HPO) of any feature
engineering and modeling pipeline is a problem discussed extensively
in the literature. HPO approaches can be broadly separated into
exhaustive and efficient optimization strategies
\cite{yang-2020-hyper-optim}. We use a two-stage procedure for
selecting the best model parameters.

The first stage is an exhaustive grid-search over diversely sampled
parameter space. Each combination of parameters instantiates a model
which is then fit to each of a set of stratified training subsets
generated by a 3-fold cross-validation strategy. Every fitted model is
subsequently tested against the cross-validation test sets and a suite
of regression scoring metrics are applied simultaneously.

The scoring metrics we choose vary by model architecture. See summary tables.

The grid search is then narrowed to a high performance quadrant of the
search space by the model evaluator based on recommendations made by a
simple entropy minimization algorithm implemented in the "yogi"
supplementary package under the yogi.model\textsubscript{selection.butler} module --
see documentation for the various grid-narrowing strategies available.

In general, the recommended grid quickly eliminates under-performing
settings based on the sample probability of a setting appearing in a
set of finalists according to the scoring rankings. The selection
score is additionally influenced by a weighted sum of the scoring
ranks allowing for considerably tuning of the selection criterion.
For best results, a few different grid spaces should be explored to
corroborate eliminations.

After the recommendation is made, the granularity of the grid is
increased in the remaining ambiguous parameters and the process is
repeated.

Additionally/Alternatively,

\subsection*{Genetic Algorithm}
\label{sec:org0f99497}
\ldots{}\\

\section*{ACKNOWLEDGMENTS}
\label{sec:org43c2c9e}
We acknowledge funding from the US Department of Energy SunShot program
under contract \#DOE DEEE005956. Use of the Center for Nanoscale
Materials, an Office of Science user facility, was supported by the U.S.
Department of Energy, Office of Science, Office of Basic Energy
Sciences, under Contract No. DE-AC02-06CH11357. We gratefully
acknowledge the computing resources provided on Bebop, a
high-performance computing cluster operated by the Laboratory Computing
Resource Center at Argonne National Laboratory. This research used
resources of the National Energy Research Scientific Computing Center, a
DOE Office of Science User Facility supported by the Office of Science
of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231.
MYT would like to acknowledge support from the U.S. Department of
Energy, Office of Science, Office of Workforce Development for Teachers
and Scientists (WDTS) under the Science Undergraduate Laboratory
Internship (SULI) program. MJD was was supported by the U. S. Department
of Energy , Office of Basic Energy Sciences, Division of Chemical
Sciences, Geosciences, and Biosciences, under Contract No.
DE-AC02-06CH11357.

\subsection*{Author Contributions}
\label{sec:org632d69c}
M.K.Y.C., R.F.K. and A.M.K. conceived the idea. A.M.K., M.Y.T. and
F.G.S. performed the DFT computations. A.M.K. and M.J.D. trained ML
models. All authors contributed to the discussion and writing of the
manuscript.

\subsection*{Data Availability}
\label{sec:org3b2687b}
DFT data and ML models are available from the corresponding author upon
reasonable request.

\subsection*{Additional Information}
\label{sec:orga742ea5}
The authors declare no competing financial or non-financial interests.

Correspondence and requests for materials should be addressed to A.M.K.
(email:amannodi@purdue.edu).

\section*{REFERENCES}
\label{sec:org53bd7fe}
\bibliographystyle{aipnum4-2}
\bibliography{../../../org/bibliotex/bibliotex}
\end{document}
