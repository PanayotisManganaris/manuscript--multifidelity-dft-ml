#+TITLE: Multi Linear Band Gap Models based on Perovskite Compositions
#+AUTHOR: Panayotis Manganaris
#+EMAIL: pmangana@purdue.edu
#+PROPERTY: header-args :session mrg :kernel mrg :async yes :pandoc org :results raw drawer
- [X] comp+prop->bg
- [ ] comp->bg
- [ ] prop->bg
- [X] exhaustive optimization
- minimal feature engineering
* Dependencies
#+INCLUDE: /home/panos/MannodiGroup/publications/Commun-Mat_DFT+ML+GA/dev_dependencies.org
#+begin_src jupyter-python
  # predictors
  from sklearn.linear_model import LinearRegression, ElasticNet, ElasticNetCV
#+end_src

* Load Data
#+INCLUDE: /home/panos/MannodiGroup/publications/Commun-Mat_DFT+ML+GA/load_full_domain.org
** focus on target 
#+begin_src jupyter-python
  target = ['bg_eV']
  my.dropna(subset=target, inplace=True)
  mm = mm.reindex(index=my.index)
#+end_src
** Scoring Scheme
*** prepare subset scoring weights and ordinal group labels
**** score on mixtype
#+begin_src jupyter-python
  mixweight = pd.get_dummies(my.mix)
  mixcat = pd.Series(OrdinalEncoder().fit_transform(my.mix.values.reshape(-1, 1)).reshape(-1),
                     index=my.index).astype(int)
#+end_src

**** score on level of theory
#+begin_src jupyter-python
  LoTweight = pd.get_dummies(my.LoT)
  LoTcat = pd.Series(OrdinalEncoder().fit_transform(my.LoT.values.reshape(-1, 1)).reshape(-1),
                     index=my.index).astype(int)
#+end_src

*** Define Scoring Metrics
**** score on mixtype
#+begin_src jupyter-python
  index_mse = PSA(mean_squared_error).score
  mix_scorings = {'r2': make_scorer(r2_score),
                  'ev': make_scorer(explained_variance_score),
                  'maxerr': make_scorer(max_error, greater_is_better=False),
                  'rmse': make_scorer(mean_squared_error, greater_is_better=False, squared=False),
                  'A_rmse': make_scorer(index_mse, greater_is_better=False,
                                        squared=False, sample_weight=mixweight.A),
                  'B_rmse': make_scorer(index_mse, greater_is_better=False,
                                        squared=False, sample_weight=mixweight.B),
                  'X_rmse': make_scorer(index_mse, greater_is_better=False,
                                        squared=False, sample_weight=mixweight.X),
                  'Pure_rmse': make_scorer(index_mse, greater_is_better=False,
                                           squared=False, sample_weight=mixweight.pure),}
#+end_src
**** score on level of theory
#+begin_src jupyter-python
  lot_scorings = {'r2': make_scorer(r2_score),
                  'ev': make_scorer(explained_variance_score),
                  'maxerr': make_scorer(max_error, greater_is_better=False),
                  'rmse': make_scorer(mean_squared_error, greater_is_better=False, squared=False),
                  'EXP_rmse': make_scorer(index_mse, greater_is_better=False,
                                          squared=False, sample_weight=LoTweight.EXP),
                  'HSE_rmse': make_scorer(index_mse, greater_is_better=False,
                                          squared=False, sample_weight=LoTweight.HSE),
                  'PBE_rmse': make_scorer(index_mse, greater_is_better=False,
                                          squared=False, sample_weight=LoTweight.PBE),
                  'HSC_rmse': make_scorer(index_mse, greater_is_better=False,
                                          squared=False, sample_weight=LoTweight.HSC),}
#+end_src

* Import Trained Model
load if available
- t :: total feature space
- c :: comp feature space
- p :: prop feature space

Never load joblib/pickle files that you do not trust, they can execute
arbitrary code on your computer.

#+begin_src jupyter-python
  cpipe = joblib.load("./Models/ols_t.joblib")
#+end_src

* Make Model
Linear models are exceedingly simple, nevertheless the whole procedure
is undertaken
** Make Pipeline
#+begin_src jupyter-python
  # composition vectors for preprocessing
  comp_features = mm.select_dtypes(np.number).columns[
      mm.select_dtypes(np.number).columns.str.match(r"\('[ABX]', '[^E]\w{,2}'\)")
  ]

  # site-avg properties for preprocessing
  prop_features = mm.select_dtypes(np.number).columns[
      ~mm.select_dtypes(np.number).columns.str.match(r"\('[ABX]', '[^E]\w{,2}'\)")
  ]

  # categorical properties for preprocessing
  cat_features = mm.select_dtypes('object').columns

  # define preprocessing
  fillna = SimpleImputer(strategy="constant", fill_value=0.0)
  mkratio = Normalizer(norm='l1')
  mknormal = StandardScaler()
  mkbound = MinMaxScaler(feature_range=(0,1), clip=False) #not statistical, should work for anything

  comp_transformer = mkpipe(fillna, mkratio, mkbound)
  prop_transformer = mkpipe(fillna, mknormal, mkbound)
  cat_transformer = ohe(handle_unknown="ignore")

  preprocessor = colt(
      transformers=[
          ("comp", comp_transformer, comp_features),
          ("prop", prop_transformer, prop_features),
          ("cat", cat_transformer, cat_features),
      ]
  )

  cpipe = mkpipe(preprocessor, LinearRegression())
#+end_src

#+begin_src jupyter-python
  with open('./Models/OLS_estimator.html', 'w') as f:  
      f.write(estimator_html_repr(cpipe))
#+end_src

** Make Dedicated Test Train Split
to ensure the fidelity training is consistent, a stratified shuffle split creates the test/train partitions
#+begin_src jupyter-python
  sss = StratifiedShuffleSplit(n_splits=1, train_size=0.8, random_state=None)
  train_idx, test_idx = next(sss.split(mm, LoTcat)) #stratify split by LoT categories
  mm_tr, mm_ts = mm.iloc[train_idx], mm.iloc[test_idx]
  my_tr, my_ts = my.iloc[train_idx], my.iloc[test_idx]
  LoTcat_tr, LoTcat_ts = LoTcat.iloc[train_idx], LoTcat.iloc[test_idx]
  mixcat_tr, mixcat_ts = mixcat.iloc[train_idx], mixcat.iloc[test_idx]
#+end_src

** Learning Curves -- Using Deterministically Random Cross Validation
#+begin_src jupyter-python
  kf_lc = KFold(n_splits=10, shuffle=True, random_state=111)
#+end_src

#+begin_src jupyter-python
  with joblib.parallel_backend('multiprocessing'):
    LC = pvc(learning_curve, cpipe, mm_tr, my_tr.bg_eV,
             train_sizes=np.linspace(0.1, 1.0, 10), cv=kf_lc,
             scoring=lot_scorings) #change scorings as needed
  LC = LC.melt(id_vars=["partition"], ignore_index=False).reset_index()
  LC = LC.groupby(['score','partition','train_sizes']).aggregate({'value':['mean','std']}).reset_index()
  LC.columns = ["".join(column) for column in LC.columns.to_flat_index()]
#+end_src
  
#+begin_src jupyter-python :file ../LearningCurves/ols_t_bg.png
  p = px.line(LC, x="train_sizes", y="valuemean", error_y='valuestd',
              facet_col="score", facet_col_wrap=3, color='partition')
  p.update_layout(hovermode='x')
  p.update_yaxes(matches=None)
  p.show(renderer='png')
#+end_src

It appears that 2-3 fold cross-validation is sufficient
- training with 400 points, validate with the compliment
  
- Validation scores cap out after about 250-350 data points
- Giving this regressor more data likely won't help.

#+begin_src jupyter-python
  p.write_html('./LearningCurves/ols_t_bg.html', include_plotlyjs='../plotly.min.js')
#+end_src
** Obtain Generality Baseline
- [X] get errors per LoT with full domain
- [ ] get errors per LoT with subdomains
  - [ ] composition only
  - [ ] properties only
  - [ ] reengineered superspace
- [X] get errors per mix with full domain
- [ ] get errors per mix with subdomains
  - [ ] composition only
  - [ ] properties only
  - [ ] reengineered superspace
*** extrapolate LoTs
#+begin_src jupyter-python 
  test_generality(cpipe, GroupKFold(n_splits=4), mix_scorings,
                  mm_tr, my_tr.bg_eV, LoTcat_tr, my_tr.LoT,
                  mm_ts, my_ts.bg_eV, LoTcat_ts, my_ts.LoT)
#+end_src

- horrible extrapolation

*** extrapolate mixes
#+begin_src jupyter-python 
  test_generality(cpipe, GroupKFold(n_splits=4), lot_scorings,
                  mm_tr, my_tr.bg_eV, mixcat_tr, my_tr.mix,
                  mm_ts, my_ts.bg_eV, mixcat_ts, my_ts.mix)
#+end_src

- extrapolations are not very accurate, but generally around 0.5 eV rmse

** HPO loop
- grid->train
- train->validate
- results->narrow
- narrow->grid
- repeat
- validation curve
** Best Model
*** Parametrize
#+begin_src jupyter-python
  grid = [
      {'linearregression__copy_X': [True],
       'linearregression__fit_intercept': [True],
       'linearregression__n_jobs': [None],
       'linearregression__normalize': ['deprecated'],
       'linearregression__positive': [False]
       }
  ]
#+end_src
*** Train Final Estimator
#+begin_src jupyter-python
  cpipe = cpipe.set_params(**{k:v[0] for k,v in grid[0].items()})
  cpipe.fit(mm_tr, my_tr.bg_eV)
#+end_src

*** evaluate
#+begin_src jupyter-python :file ../ParityPlots/ols_t_bg.png
  #change between tr and ts suffixes to see test vs train pairity plot
  p, data = parityplot(cpipe, mm_tr, my_tr.bg_eV.to_frame(), color=mm_tr.LoT)
  p.show(renderer='png')
#+end_src
  
#+begin_src jupyter-python 
  p.write_html('./ParityPlots/ols_t_bg.html', include_plotlyjs='../plotly.min.js')
#+end_src

#+begin_src jupyter-python
  pd.Series(batch_score(cpipe, mm_ts, my_ts.bg_eV, **mix_scorings)).to_frame()
#+end_src

#+begin_src jupyter-python
  pd.Series(batch_score(cpipe, mm_ts, my_ts.bg_eV, **lot_scorings)).to_frame()
#+end_src

*** discussion
The linear model is actually pretty reasonable, but it's unlikely that
it's learning anything very fundamental about the underlying physics
of the Pervoskite system. however, that doesn't mean it reveals nothing.

#+begin_src jupyter-python
  interpret = pd.DataFrame(
      cpipe[-1].coef_,
      index=cpipe[:-1].get_feature_names_out(),
      columns=['coefficients']
  )
  interpret.index = interpret.index.str.replace(pat=r"\[|'|\]", repl="", regex=True)
  interpret#[interpret!=0]
#+end_src

#+begin_src jupyter-python 
  interpret.groupby(interpret.index.str[4:8]).aggregate(lambda x: np.sqrt(sum(x**2)))
#+end_src
* Export Trained Model
#+begin_src jupyter-python
  joblib.dump(cpipe, "./Models/ols_t.joblib")
#+end_src

* Import Trained Model
load if available
#+begin_src jupyter-python
  cpipe = joblib.load("./Models/en_t.joblib")
#+end_src
* Make Model
The features used here are proportions of a total. They are
correlated. So, it is worth using a more intelligent linear regression
to try extracting an indication of the most impactful features.

ElasticNet uses Lasso in combination with l1 and l2 regularization of
the model parameters to encourage sparsity. however, it is not
especially strict. unlike Lasso (another sparsifying linear
regressor), ElasticNet will use multiple sets of correlated features
instead of -- effectively randomly -- picking only one in the effort
to return sparse coefficients.
** Make Pipeline
#+begin_src jupyter-python
  # composition vectors for preprocessing
  comp_features = mm.select_dtypes(np.number).columns[
      mm.select_dtypes(np.number).columns.str.match(r"\('[ABX]', '[^E]\w{,2}'\)")
  ]

  # site-avg properties for preprocessing
  prop_features = mm.select_dtypes(np.number).columns[
      ~mm.select_dtypes(np.number).columns.str.match(r"\('[ABX]', '[^E]\w{,2}'\)")
  ]

  # categorical properties for preprocessing
  cat_features = mm.select_dtypes('object').columns

  # define preprocessing
  fillna = SimpleImputer(strategy="constant", fill_value=0.0)
  mkratio = Normalizer(norm='l1')
  mknormal = StandardScaler()
  mkbound = MinMaxScaler(feature_range=(0,1), clip=False) #not statistical, should work for anything

  comp_transformer = mkpipe(fillna, mkratio, mkbound)
  prop_transformer = mkpipe(fillna, mknormal, mkbound)
  cat_transformer = ohe(handle_unknown="ignore")

  preprocessor = colt(
      transformers=[
          ("comp", comp_transformer, comp_features),
          ("prop", prop_transformer, prop_features),
          ("cat", cat_transformer, cat_features),
      ]
  )

  cpipe = mkpipe(preprocessor, ElasticNetCV(max_iter=2000))
#+end_src

** Make Dedicated Test Train Split
#+begin_src jupyter-python
  sss = StratifiedShuffleSplit(n_splits=1, train_size=0.8, random_state=None)
  train_idx, test_idx = next(sss.split(mm, LoTcat)) #stratify split by LoT categories
  mm_tr, mm_ts = mm.iloc[train_idx], mm.iloc[test_idx]
  my_tr, my_ts = my.iloc[train_idx], my.iloc[test_idx]
  mix_tr, mix_ts = mix.iloc[train_idx], mix.iloc[test_idx]
  LoTcat_tr, LoTcat_ts = LoTcat.iloc[train_idx], LoTcat.iloc[test_idx]
  mixcat_tr, mixcat_ts = mixcat.iloc[train_idx], mixcat.iloc[test_idx]
#+end_src

** Learning Curves -- Using Deterministically Random Cross Validation
#+begin_src jupyter-python
  kf_lc = KFold(n_splits=10, shuffle=True, random_state=111)
#+end_src

#+begin_src jupyter-python
  with joblib.parallel_backend('multiprocessing'):
    LC = pvc(learning_curve, cpipe, mm_tr, my_tr.bg_eV,
             train_sizes=np.linspace(0.1, 1.0, 10), cv=kf_lc,
             scoring=lot_scorings) #change scorings as needed
  LC = LC.melt(id_vars=["partition"], ignore_index=False).reset_index()
  LC = LC.groupby(['score','partition','train_sizes']).aggregate({'value':['mean','std']}).reset_index()
  LC.columns = ["".join(column) for column in LC.columns.to_flat_index()]
#+end_src
  
#+begin_src jupyter-python :file ../LearningCurves/ols_t_bg.png
  p = px.line(LC, x="train_sizes", y="valuemean", error_y='valuestd',
              facet_col="score", facet_col_wrap=3, color='partition')
  p.update_layout(hovermode='x')
  p.update_yaxes(matches=None)
  p.show(renderer='png')
#+end_src

It appears that 2-3 fold cross-validation is sufficient
- training with 400 points, validate with the compliment
  
- Validation scores cap out after about 250-350 data points
- Giving this regressor more data likely won't help.

#+begin_src jupyter-python
  p.write_html('./LearningCurves/en_t_bg.html', include_plotlyjs='../plotly.min.js')
#+end_src
** Obtain Generality Baseline
- [X] get errors per LoT with full domain
- [ ] get errors per LoT with subdomains
  - [ ] composition only
  - [ ] properties only
  - [ ] reengineered superspace
- [X] get errors per mix with full domain
- [ ] get errors per mix with subdomains
  - [ ] composition only
  - [ ] properties only
  - [ ] reengineered superspace
*** extrapolate LoTs
#+begin_src jupyter-python 
  test_generality(cpipe, GroupKFold(n_splits=4), mix_scorings,
                  mm_tr, my_tr.bg_eV, LoTcat_tr, my_tr.LoT,
                  mm_ts, my_ts.bg_eV, LoTcat_ts, my_ts.LoT)
#+end_src

- extrapolations are not very accurate, but massively outperform ols model

*** extrapolate mixes
#+begin_src jupyter-python 
  test_generality(cpipe, GroupKFold(n_splits=4), lot_scorings,
                  mm_tr, my_tr.bg_eV, mixcat_tr, my_tr.mix,
                  mm_ts, my_ts.bg_eV, mixcat_ts, my_ts.mix)
#+end_src

- extrapolations are not very accurate, but massively outperform ols model
** HPO loop
- grid->train
- train->validate
- results->narrow
- narrow->grid
- repeat
- validation curve
** Best Model
*** Parametrize
#+begin_src jupyter-python
  grid = [
      {'elasticnetcv__alphas': [None],
       'elasticnetcv__copy_X': [True],
       'elasticnetcv__cv': [5], #default, good enough by LC
       'elasticnetcv__eps': [0.001], #steps/1.0 in alpha optimization
       'elasticnetcv__fit_intercept': [True],
       'elasticnetcv__l1_ratio': [0.5],
       'elasticnetcv__max_iter': [2000],
       'elasticnetcv__n_alphas': [100],
       'elasticnetcv__n_jobs': [None],
       'elasticnetcv__normalize': ['deprecated'],
       'elasticnetcv__positive': [False],
       'elasticnetcv__precompute': ['auto'],
       'elasticnetcv__random_state': [None],
       'elasticnetcv__selection': ['cyclic'],
       'elasticnetcv__tol': [0.0001],
       'elasticnetcv__verbose': [0]
       }
  ]
#+end_src

*** Train Final Estimator
#+begin_src jupyter-python
  cpipe = cpipe.set_params(**{k:v[0] for k,v in grid[0].items()})
  cpipe.fit(mm_tr, my_tr.bg_eV)
#+end_src

*** evaluate
#+begin_src jupyter-python :file ../ParityPlots/en_t_bg.png
  #change between tr and ts suffixes to see test vs train pairity plot
  p, data = parityplot(cpipe, mm_ts, my_ts.bg_eV.to_frame(), color=mm_ts.LoT)
  p.show(renderer='png')
#+end_src
  
#+begin_src jupyter-python 
  p.write_html('./ParityPlots/en_t_bg.html', include_plotlyjs='../plotly.min.js')
#+end_src


#+begin_src jupyter-python
  pd.Series(batch_score(cpipe, mm_ts, my_ts.bg_eV, **mix_scorings)).to_frame()
#+end_src

#+begin_src jupyter-python
  #change between tr and ts suffixes to see test vs train scores -- both are good
  pd.Series(batch_score(cpipe, mm_ts, my_ts.bg_eV, **scorings)).to_frame()
#+end_src

:end:

*** discussion
#+begin_src jupyter-python
  interpret = pd.DataFrame(
      cpipe[-1].coef_,
      index=cpipe[:-1].get_feature_names_out(),
      columns=['coefficients']
  )
  interpret.index = interpret.index.str.replace(pat=r"\[|'|\]", repl="", regex=True)
  interpret#[interpret!=0]
#+end_src

#+begin_src jupyter-python 
  interpret.groupby(interpret.index.str[4:8]).aggregate(lambda x: np.sqrt(sum(x**2)))
#+end_src

* Export Trained Model
#+begin_src jupyter-python
  joblib.dump(cpipe, "./Models/en_t.joblib")
#+end_src
